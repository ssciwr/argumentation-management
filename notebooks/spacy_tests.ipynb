{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69223af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ein paar versuche\n",
    "\n",
    "enable = [\"parser\", \"ner\", \"senter\"]\n",
    "# um basierend auf den \"aktiven\" Komponenten auf die \"inaktiven\" zu schließen müssen\n",
    "# müssen wir die pipeline einmal komplett laden\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "components = [component[0] for component in nlp.components]\n",
    "\n",
    "exclude = [component for component in components if component not in enable]\n",
    "\n",
    "# ermitteln welche Komponenten nicht geladen werden\n",
    "# exclude = [proc for proc in nlp.pipe_names if proc not in enable]\n",
    "# print(exclude)\n",
    "\n",
    "config = {\n",
    "    \"name\": \"en_core_web_sm\",\n",
    "    \"disable\": [],\n",
    "    \"exclude\": exclude,\n",
    "    \"config\": {\n",
    "        \"nlp.batch_size\": 256,\n",
    "        \"components\": {\n",
    "            \"attribute_ruler\": {\"validate\": False},\n",
    "            \"lemmatizer\": {\"mode\": \"rule\", \"model\": None, \"overwrite\": False},\n",
    "            \"parser\": {\n",
    "                \"moves\": None,\n",
    "                \"min_action_freq\": 12,\n",
    "                \"learn_tokens\": False,\n",
    "                \"update_with_oracle_cut_size\": 200,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# tmp = config.copy()\n",
    "# da wir die pipeline haben können wir aus der Config spezifisch nur die Modifikationen laden die\n",
    "# für das gegebene Model zulässig sind\n",
    "# poplist = [key for key in config[\"config\"][\"components\"] if key not in nlp.pipe_names]\n",
    "# for key in poplist:\n",
    "#    tmp[\"config\"][\"components\"].pop(key)\n",
    "\n",
    "nlp = spacy.load(**config)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "text = \"This is a sentence. This is yet another sentence.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# for token in doc:\n",
    "#    print(token.text, token.lemma_, token.norm_)\n",
    "tmp = []\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4b998",
   "metadata": {},
   "source": [
    "# Test on text from tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c065c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_text(path):\n",
    "    data = []\n",
    "    i = 0\n",
    "    inpar = False\n",
    "    with open(path, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            if line.startswith(\"<\"):\n",
    "                if inpar is False:\n",
    "                    inpar = True\n",
    "                    data.append([\"\", \"\", \"\"])\n",
    "                    data[i][0] += line.replace(\"\\n\", \" \")\n",
    "                elif inpar is True:\n",
    "                    inpar = False\n",
    "                    data[i][2] += line.replace(\"\\n\", \" \")\n",
    "                    i += 1\n",
    "            elif inpar:\n",
    "                data[i][1] += line.replace(\"\\n\", \" \")\n",
    "    return data\n",
    "\n",
    "\n",
    "data = get_sample_text(\"../data/Original/plenary.vrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f397fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "\n",
    "for elem in data:\n",
    "    print(elem[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc1 = nlp(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc1.sents:\n",
    "    for token in sent:\n",
    "        print(token.i, token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ab803",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-approach",
   "metadata": {},
   "source": [
    "# Sentencizer\n",
    "https://spacy.io/usage/linguistic-features#sbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "assert doc.has_annotation(\"SENT_START\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"../data/Original/iued_test_original.txt\"\n",
    "# name = \"../data/Original/iued_test_original.vrt\"\n",
    "with open(name, \"r\") as myfile:\n",
    "    data = myfile.read().replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cd31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "assert doc.has_annotation(\"SENT_START\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print(\"***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde50776",
   "metadata": {},
   "source": [
    "This gives somewhat accurate results, with some errors after numbers. You can also use a trained model, however this will not work on uncommon texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print(\"***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9379e30",
   "metadata": {},
   "source": [
    "Also fails for the example here. Then there is the one based on a statistical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92539652",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.enable_pipe(\"senter\")\n",
    "doc = nlp(data)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print(\"***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b28c9",
   "metadata": {},
   "source": [
    "Directly use the sentencizer without the pipeline - this one looks at punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()  # just the language with no pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "doc = nlp(data)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print(\"***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b81e27",
   "metadata": {},
   "source": [
    "Seems to work correctly. What is the difference to the pipeline? In the DW scripts, the other components are disabled via the \"exclude\" command - should be faster as pipeline is not loaded at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for doc in nlp.pipe(\n",
    "    texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]\n",
    "):\n",
    "    # Do something with the doc here\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c752ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\", exclude=[\"tagger\", \"ner\", \"attribute_ruler\", \"lemmatizer\"]\n",
    ")\n",
    "for doc in nlp.pipe(texts):\n",
    "    for sent in doc.sents:\n",
    "        print(sent.text)\n",
    "        print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0217e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = spacy.blank(\"en\")\n",
    "for key in lang.factories:\n",
    "    print(key, English.factories[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# print(nlp.components)\n",
    "used = [\"tok2vec\", \"ner\"]\n",
    "disable = []\n",
    "for tupple in nlp.components:\n",
    "    if tupple[0] not in used:\n",
    "        disable.append(tupple[0])\n",
    "\n",
    "print(disable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1946ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = \"ner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca31d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if factory in nlp.factories:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cea49",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "https://spacy.io/usage/linguistic-features#tokenization  \n",
    "We need to allow for special case rules. \n",
    "```\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "```\n",
    "\n",
    "Also, there are custom tokenizer libraries that one may want to load. Probably we would want to keep it so that users can specify their custom tokenizers in addition to the standard one from spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7cc315",
   "metadata": {},
   "source": [
    "# Lemmatizer\n",
    "https://spacy.io/usage/linguistic-features#lemmatization\n",
    "\n",
    "needs package spacy_lookups_data to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nlp.add_pipe(\n",
    "    \"lemmatizer\"\n",
    ")  # need to be carefull which components are already in the pipeline or not. get_pipe() throws me an error when running this from the top\n",
    "print(lemmatizer.mode)  # 'rule'\n",
    "lemmatizer.initialize(lookups=None)\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d46ee9",
   "metadata": {},
   "source": [
    "Should punctuation be excluded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc49f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da57ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f663e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936093af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b36cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca13b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec81832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7996d52f",
   "metadata": {},
   "source": [
    "# POS tagger\n",
    "https://spacy.io/usage/linguistic-features#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c539fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\n",
    "        token.text,\n",
    "        token.lemma_,\n",
    "        token.pos_,\n",
    "        token.tag_,\n",
    "        token.dep_,\n",
    "        token.shape_,\n",
    "        token.is_alpha,\n",
    "        token.is_stop,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1c396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352b174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b03cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083bb02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9aa50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef38f8fb",
   "metadata": {},
   "source": [
    "# Morphology\n",
    "https://spacy.io/usage/linguistic-features#morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "token = doc[0]  # 'I'\n",
    "print(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\n",
    "print(token.morph.get(\"PronType\"))  # ['Prs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62417153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5703ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c09da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a0821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddb1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca38ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4febe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "170975a1",
   "metadata": {},
   "source": [
    "# Constituency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde5927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79e0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47151e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd32496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f3d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb6d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc60eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4c91c3",
   "metadata": {},
   "source": [
    "# Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadd6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e02546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2eb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3e222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2927c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2bd493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a39d2f89",
   "metadata": {},
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c4b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f2521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88601695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6c99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f66e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a3dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0237d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e4d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332f747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c307d87",
   "metadata": {},
   "source": [
    "# Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0728e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330c287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e34f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8d8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ecdb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c76f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660fed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c01c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028de5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10a95b83",
   "metadata": {},
   "source": [
    "# Named entities\n",
    "\n",
    "Needs a spaCy pipeline that supports entity recognition. After running the pipeline the named entities can be accesed via Doc.ents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61bf466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcda4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our case\n",
    "doc = nlp(data)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start, ent.end, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities_spacy(doc):\n",
    "    \"\"\"Fetch named entities from spaCy Doc object.\n",
    "\n",
    "    [Args]:\n",
    "            doc[class object]: The doc object of the data of interest after application of a spaCy pipeline.\n",
    "\n",
    "    [Returns]:\n",
    "            [dict]: Dictionary containing the identified named entities by name and type of label given by\n",
    "                    spaCy associated with a list containing the indices to the start and end char for each\n",
    "                    individual instance.\"\"\"\n",
    "\n",
    "    # define defaultdict to store the named entities\n",
    "    named_entities = defaultdict(list)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        # add the entities label, start_char and end_char to the dictionary\n",
    "        named_entities[\n",
    "            \"Text: {} |Label: {} |IOB: {}\".format(ent.text, ent.label_, ent.label)\n",
    "        ].append([ent.start_char, ent.end_char])\n",
    "\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce7d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_ent = named_entities_spacy(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(named_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa21cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in named_ent[\"Text: Audi |Label: ORG |IOB: 383\"]:\n",
    "    print(data[idx[0] - 20 : idx[1] + 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31568bfc",
   "metadata": {},
   "source": [
    "# Unique Tokens or smthg :)\n",
    "\n",
    "\n",
    "Nicht das was named entities eigentlich will...\n",
    "Für einzelne Tokens: [Matcher](https://spacy.io/usage/rule-based-matching) \\\n",
    "Für ganze Sätze: [Phrasematcher](https://spacy.io/usage/rule-based-matching#phrasematcher) \\\n",
    "Ich schätze für den Moment sind wir nur an einzelnen Tokens interessiert? Oder an allen einzigartigen Token im ganzen Text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b43ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# this works on the same example as above\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# initialize the matcher, the vocab has to be the same as for the text\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# idealy the user specifies what he wants to search and what attribute to assign?\n",
    "terms = [[{\"LOWER\": \"audi\"}], [{\"LOWER\": \"improvements\"}], [{\"LOWER\": \"parking\"}]]\n",
    "\n",
    "# also supports regular expressions:\n",
    "terms += [[{\"TEXT\": {\"REGEX\": r\"^[Ii](\\.?|f)$\"}}]]  # search for I, i, If, if\n",
    "\n",
    "print(\"Query: {}\".format(terms))\n",
    "\n",
    "# add the terms to look for to the mathcer\n",
    "matcher.add(\"Query\", terms)\n",
    "\n",
    "# load the data into doc\n",
    "doc = nlp(data)\n",
    "# run the matcher on the text in doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# get the indices (would correspond to corpus position from cwb?)\n",
    "indices = [[start, end] for _, start, end in matches]\n",
    "\n",
    "print(indices)\n",
    "\n",
    "dict_out = defaultdict(\n",
    "    list\n",
    ")  # default dict initializes the value of a new key that is added with an empty list\n",
    "# which we can then append to\n",
    "\n",
    "# put the found indices to access the searched terms in a dictionary where they are available via said terms\n",
    "# We have to go through all the found entities to confirm what term they correspond to...\n",
    "# For large texts where we have many hits faster to split the query beforehand? Maybe parallel searching?\n",
    "for index_pair in indices:\n",
    "    dict_out[\"{}\".format(doc[index_pair[0] : index_pair[1]])].append(index_pair)\n",
    "\n",
    "# display the output\n",
    "for key in dict_out:\n",
    "    print(\"{} found at location {}.\".format(key, dict_out[key]))\n",
    "\n",
    "# for index in indices:\n",
    "#    print('{} found at index location {}'.format(doc[index], index))\n",
    "\n",
    "# print(matches)\n",
    "\n",
    "# for match_id, start, end in matches:\n",
    "#    string_id = nlp.vocab.strings[match_id] #Get string representation\n",
    "#    span = doc[start:end] # matched span\n",
    "#    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also search for words of certain length or above/below certain lengths\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [[{\"LENGTH\": {\"==\": 10}}]]  # , [{\"LENGTH\":{\"<=\":1}}], [{\"LENGTH\":{\">=\":12}}]]\n",
    "\n",
    "matcher.add(\"Query\", pattern)\n",
    "matches = matcher(doc)\n",
    "\n",
    "indices = [[start, end] for _, start, end in matches]\n",
    "\n",
    "dict_out = defaultdict(list)\n",
    "\n",
    "for index_pair in indices:\n",
    "    dict_out[\"{}\".format(doc[index_pair[0] : index_pair[1]])].append(index_pair)\n",
    "\n",
    "for key in dict_out:\n",
    "    print(\"{} found at location {}.\".format(key, dict_out[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for token pattern\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# seach for the different types of cars with a \"wildcard token pattern\" leaving the last token empty\n",
    "pattern = [[{\"ORTH\": \"Audi\"}, {\"ORTH\": \"A\"}, {}]]\n",
    "\n",
    "matcher.add(\"Query\", pattern)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "indices = [[start, end] for _, start, end in matches]\n",
    "\n",
    "for _, start, end in matches:\n",
    "    indices.append([start, end])\n",
    "\n",
    "dict_out = defaultdict(list)\n",
    "\n",
    "for index_pair in indices:\n",
    "    dict_out[\"{}\".format(doc[index_pair[0] : index_pair[1]])].append(index_pair)\n",
    "\n",
    "for key in dict_out:\n",
    "    print(\"{} found at location {}.\".format(key, dict_out[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it in a function\n",
    "\n",
    "\n",
    "def search_text(query, nlp, doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    matcher.add(\"Query\", query)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    indices = [[start, end] for _, start, end in matches]\n",
    "\n",
    "    dict_out = defaultdict(list)\n",
    "\n",
    "    for index_pair in indices:\n",
    "        dict_out[\"{}\".format(doc[index_pair[0] : index_pair[1]])].append(index_pair)\n",
    "\n",
    "    # for key in dict_out:\n",
    "    #    print(\"{} found at location {}.\".format(key, dict_out[key]))\n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [[{\"ORTH\": \"Audi\"}, {\"ORTH\": \"A\"}, {}]]\n",
    "\n",
    "test1 = search_text(query, nlp, doc)\n",
    "\n",
    "print(test1)\n",
    "\n",
    "print(\"*\" * 50)\n",
    "\n",
    "# can just add different queries together\n",
    "query += terms\n",
    "\n",
    "test2 = search_text(query, nlp, doc)\n",
    "\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c897f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(doc):\n",
    "    \"\"\"Get number of unique words in doc\"\"\"\n",
    "\n",
    "    seen = set()\n",
    "    for token in doc:\n",
    "        if token.text not in seen:\n",
    "            seen.add(token.text)\n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_token(query, nlp, doc):\n",
    "    \"\"\"search text for specific token and return all the found locations in dict.\"\"\"\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    matcher.add(\"Query\", query)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    indices = [[start, end] for _, start, end in matches]\n",
    "\n",
    "    dict_out = defaultdict(list)\n",
    "\n",
    "    dict_out[\"{}\".format(doc[indices[0][0] : indices[0][1]])].append(indices)\n",
    "\n",
    "    # for key in dict_out:\n",
    "    #    print(\"{} found at location {}.\".format(key, dict_out[key]))\n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_tokens(nlp, doc):\n",
    "    \"\"\"Get locations for all umique words in doc into a dictionary, case sensitive\"\"\"\n",
    "\n",
    "    unique_tokens = None\n",
    "\n",
    "    # get the number of unique tokens in text, so we don't index twice\n",
    "    unique = get_unique(doc)\n",
    "\n",
    "    for token in unique:\n",
    "\n",
    "        if unique_tokens:\n",
    "            # if the dictionary is not empty:\n",
    "            if token in unique_tokens:\n",
    "                # if the token is already in the dictionary:\n",
    "                pass\n",
    "            elif token not in unique_tokens:\n",
    "                # if the token is not already in there add it:\n",
    "                unique_tokens.update(\n",
    "                    search_token([[{\"ORTH\": \"{}\".format(token)}]], nlp, doc)\n",
    "                )\n",
    "        else:\n",
    "            # if the dictionary hasn't been initialized do so with first token\n",
    "            unique_tokens = search_token([[{\"ORTH\": \"{}\".format(token)}]], nlp, doc)\n",
    "\n",
    "        if len(unique_tokens) == unique:\n",
    "            # if we have passed each unique word already there is no need to continue\n",
    "            break\n",
    "\n",
    "    return unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tok = unique_tokens(nlp, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in named_ent:\n",
    "    print(\"{}: {}\".format(key, named_ent[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32668878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(named_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(named_ent[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "int = input(\"Could not find on system. Attempt to download? [Y/N]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc8bdf",
   "metadata": {},
   "source": [
    "# Basically just old unused stuff from class module beyond this point, but you never know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c442ebe",
   "metadata": {},
   "source": [
    "    #def grab_NER(self):\n",
    "\n",
    "        # store the named entities and associated information about label\n",
    "    #    self.named_entities = dict()\n",
    "        # extract the information such as Label (as text and IOB code)\n",
    "        # and position in text (by start/end char and start/end text ID)\n",
    "        # the ID should correspond to the id in XML i think\n",
    "        #for ent in self.doc.ents:\n",
    "            # if entity is one token we just append it\n",
    "            #if ent.end - ent.start == 1:\n",
    "                #self.named_entities[\"{}\".format(ent.start)] = {\n",
    "                    #\"Label_text\": ent.label_,\n",
    "                    #\"IOB\": ent.label,\n",
    "                    #\"Chars\": [ent.start_char, ent.end_char],\n",
    "                    #\"Token Ids\": [ent.start, ent.end],\n",
    "                    #\"Text\": ent.text,\n",
    "                #}\n",
    "            # if entity is more than one token we also add the Ids of\n",
    "            # all contained tokens to the dict and assign them the same labels etc.\n",
    "            #else:\n",
    "                #for i in range(ent.start, ent.end):\n",
    "                    #self.named_entities[\"{}\".format(i)] = {\n",
    "                        #\"Label_text\": ent.label_,\n",
    "                        #\"IOB\": ent.label,\n",
    "                        #\"Chars\": [ent.start_char, ent.end_char],\n",
    "                        #\"Token Ids\": [ent.start, ent.end],\n",
    "                        #\"Text\": ent.text,\n",
    "                    #}\n",
    "\n",
    "        #return self.named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5239c3",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "def preprocess(spec_pipe=None):\n",
    "    \"\"\"Load model\"\"\"\n",
    "\n",
    "    # Add possibility for user to provide their own spacy model or\n",
    "    # use specific one?\n",
    "    if spec_pipe:\n",
    "        nlp = sp.load(spec_pipe)\n",
    "\n",
    "    else:\n",
    "        # just this one model for now, what models are needed? There\n",
    "        # are a lot of different ones in the old preprocessing\n",
    "        print(\"Loading model en_core_web_sm\")\n",
    "        nlp = sp.load(\"en_core_web_sm\")\n",
    "\n",
    "    return nlp\n",
    "\n",
    "\n",
    "def pipeline_with_dict():\n",
    "\n",
    "    # Information about pipeline and the components:\n",
    "    # https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "    config = {\n",
    "        \"name\":\"en_core_web_sm\", #name of model\n",
    "        \"disable\":[], # components to be disabled, given as list, loaded but not used\n",
    "        \"exclude\":[\"tagger\",\"parser\",\"attribute_ruler\",\"lemmatizer\"], # components to be excluded, given as list, wont be loaded\n",
    "        \"config\":{}\n",
    "    }# put the instructions for sp.laod() into dict\n",
    "\n",
    "    nlp = sp.load(**config)\n",
    "    return nlp\n",
    "\n",
    "\n",
    "def blank_with_dict():\n",
    "    # load a default language and then add components from a pretrained pipeline for\n",
    "    # that language\n",
    "    config = {\n",
    "        \"lang\":\"en\",\n",
    "        \"add_to\":[\"tok2vec\", \"tagger\",\"parser\",\"ner\",\"attribute_ruler\",\"lemmatizer\"],\n",
    "        \"source\":\"en_core_web_sm\"\n",
    "    }# config equivalent to sp.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "    configNER = {\n",
    "        \"lang\":\"en\",\n",
    "        \"add_to\":[\"ner\"],\n",
    "        \"source\":\"en_core_web_sm\"\n",
    "    }# load just NER from en_core_web_sm\n",
    "\n",
    "    nlp = sp.blank(configNER[\"lang\"])\n",
    "    for factory in configNER[\"add_to\"]:\n",
    "        nlp.add_pipe(factory, source=sp.load(configNER[\"source\"]))\n",
    "\n",
    "    return nlp\n",
    "\n",
    "def apply_pipe(data, nlp):\n",
    "    \"\"\"Apply given pipeline to data\"\"\"\n",
    "    # for multiprocessing or if we have multiple documents we can consider\n",
    "    # something like:\n",
    "    # docs = [doc for doc in nlp.pipe(data, n_process=cores)]\n",
    "    # this should stream the individual documents and create Docs in order while\n",
    "    # working in parallel. Maybe break down large corpus into peaces for this?\n",
    "    return nlp(data)\n",
    "\n",
    "\n",
    "# I guess we want to eiter load a pipeline or build one and apply it\n",
    "# so we can assume that doc exists and if NER is called, that\n",
    "# the pipeline included 'ner'\n",
    "def NER(doc):\n",
    "    \"\"\"Get the NER tokens from Doc\"\"\"\n",
    "\n",
    "    # store the named entities and associated information about label\n",
    "    named_entities = defaultdict(list)\n",
    "    # extract the information such as Label (as text and IOB code)\n",
    "    # and position in text (by start/end char and start/end text ID)\n",
    "    # the ID should correspond to the id in XML i think\n",
    "    for ent in doc.ents:\n",
    "        named_entities[\"Start Id: {}\".format(ent.start)].append(\n",
    "            {\n",
    "                \"Label_text\": ent.label_,\n",
    "                \"IOB\": ent.label,\n",
    "                \"Chars\": [ent.start_char, ent.end_char],\n",
    "                \"Token Ids\": [ent.start, ent.end],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return named_entities\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"../../data/Original/iued_test_original.txt\", \"r\") as file:\n",
    "        data = file.read().replace(\"\\n\", \"\")\n",
    "\n",
    "    # use the regular model\n",
    "    doc = apply_pipe(data, preprocess())\n",
    "    # for token in doc:\n",
    "    #   print(token.text)\n",
    "    named_ents = NER(doc)\n",
    "    for key in named_ents:\n",
    "        print(\"{} {}\".format(key, named_ents[key]))\n",
    "\n",
    "    # also use regular model but exclude components\n",
    "    doc2 = apply_pipe(data, pipeline_with_dict())\n",
    "    print('*'*20)\n",
    "    # get NER\n",
    "    named_ents2 = NER(doc2)\n",
    "    # check that NER still works\n",
    "    for key in named_ents:\n",
    "        print(\"{} {}\".format(key, named_ents2[key]))\n",
    "        if key in named_ents2:\n",
    "            try:\n",
    "                assert named_ents2[key] == named_ents[key]\n",
    "            except AssertionError:\n",
    "                print(named_ents[key], named_ents2[key])\n",
    "        else:\n",
    "            print(\"!Untracked token in ents2: {}\".format(named_ents[key]))\n",
    "\n",
    "    # \"rebuilt\" en_core_web_sm manually from basic sp.lang.en.English\n",
    "    doc3 = apply_pipe(data, blank_with_dict())\n",
    "    print('*'*20)\n",
    "    named_ents3 = NER(doc3)\n",
    "    # check that NER still works\n",
    "    for key in named_ents3:\n",
    "        print(\"{} {}\".format(key, named_ents3[key]))\n",
    "        if key in named_ents3:\n",
    "            try:\n",
    "                assert named_ents3[key] == named_ents[key]\n",
    "            except AssertionError:\n",
    "                print(named_ents[key], named_ents3[key])\n",
    "        else:\n",
    "            print(\"!Untracked token in ents3: {}\".format(named_ents[key]))\n",
    "    # -> NER still works fine, but labels one entity differently for some reason\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae1872",
   "metadata": {},
   "source": [
    "  #for sent in doc.sents:\n",
    "    #    print(\"<\\s>\")\n",
    "    #    for token in sent:\n",
    "    #        print(token.i, token.text, token.ent_type_)\n",
    "    #    print(\"<\\s>\")\n",
    "    \n",
    "    # either load pipe with config from above or load a complete pretrained pipe\n",
    "    # this will load all components\n",
    "    #results = spaCy_pipe(config).apply_to(data).grab_results()\n",
    "    # , pretrained=\"en_core_web_sm\")\n",
    "\n",
    "    #for key, value in results[\"ner\"].items():\n",
    "    #    print(\n",
    "    #        \"ID {} : [IOB]= {} [Text]={}, [Label]={}\".format(\n",
    "    #            key, value[\"IOB\"], value[\"Text\"], value[\"Label_text\"]\n",
    "    #        )\n",
    "    #    )\n",
    "\n",
    "    #print(\"*\" * 50)\n",
    "\n",
    "    #different_config = {\n",
    "    #        \"config\":{\"lang\": \"en\", \"add_to\": [\"ner\"], \"source\": \"en_core_web_md\"},\n",
    "    #        \"jobs\": [\"ner\"],\n",
    "    #        \"pretrained\": False\n",
    "    #    }\n",
    "\n",
    "    #results1 = spaCy_pipe(different_config).apply_to(data).grab_results()\n",
    "\n",
    "    # print(results1)\n",
    "\n",
    "    #for key, value in results1[\"ner\"].items():\n",
    "    #    print(\n",
    "    #        \"ID {} : [IOB]= {} [Text]={}, [Label]={}\".format(\n",
    "    #            key, value[\"IOB\"], value[\"Text\"], value[\"Label_text\"]\n",
    "    #        )\n",
    "    #    )\n",
    "\n",
    "        #def grab_results(self):\n",
    "\n",
    "        # grab the results after running the pipeline, currently only for NER\n",
    "    #    results = dict()\n",
    "    #    if \"ner\" in self.jobs:\n",
    "    #        results[\"ner\"] = self.grab_NER()\n",
    "\n",
    "    #    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9e706",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
