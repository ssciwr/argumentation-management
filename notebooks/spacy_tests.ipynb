{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-approach",
   "metadata": {},
   "source": [
    "# Sentencizer\n",
    "https://spacy.io/usage/linguistic-features#sbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "assert doc.has_annotation(\"SENT_START\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"../data/Original/iued_test_original.txt\"\n",
    "#name = \"../data/Original/iued_test_original.vrt\"\n",
    "with open (name, \"r\") as myfile:\n",
    "    data=myfile.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cd31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "assert doc.has_annotation(\"SENT_START\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print('***')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde50776",
   "metadata": {},
   "source": [
    "This gives somewhat accurate results, with some errors after numbers. You can also use a trained model, however this will not work on uncommon texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print('***')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9379e30",
   "metadata": {},
   "source": [
    "Also fails for the example here. Then there is the one based on a statistical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92539652",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.enable_pipe(\"senter\")\n",
    "doc = nlp(data)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print('***')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b28c9",
   "metadata": {},
   "source": [
    "Directly use the sentencizer without the pipeline - this one looks at punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()  # just the language with no pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "doc = nlp(data)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print('***')            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b81e27",
   "metadata": {},
   "source": [
    "Seems to work correctly. What is the difference to the pipeline? In the DW scripts, the other components are disabled via the \"exclude\" command - should be faster as pipeline is not loaded at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    # Do something with the doc here\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c752ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"tagger\", \"ner\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "for doc in nlp.pipe(texts):\n",
    "    for sent in doc.sents:\n",
    "        print(sent.text)\n",
    "        print('***') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0217e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0f812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1946ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca31d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d32cea49",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "https://spacy.io/usage/linguistic-features#tokenization  \n",
    "We need to allow for special case rules. \n",
    "```\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "```\n",
    "\n",
    "Also, there are custom tokenizer libraries that one may want to load. Probably we would want to keep it so that users can specify their custom tokenizers in addition to the standard one from spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7cc315",
   "metadata": {},
   "source": [
    "# Lemmatizer\n",
    "https://spacy.io/usage/linguistic-features#lemmatization\n",
    "\n",
    "needs package spacy_lookups_data to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nlp.add_pipe(\"lemmatizer\")  # need to be carefull which components are already in the pipeline or not. get_pipe() throws me an error when running this from the top\n",
    "print(lemmatizer.mode)  # 'rule'\n",
    "lemmatizer.initialize(lookups=None)\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d46ee9",
   "metadata": {},
   "source": [
    "Should punctuation be excluded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc49f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da57ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f663e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936093af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b36cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca13b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec81832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7996d52f",
   "metadata": {},
   "source": [
    "# POS tagger\n",
    "https://spacy.io/usage/linguistic-features#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c539fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1c396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352b174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b03cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083bb02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9aa50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef38f8fb",
   "metadata": {},
   "source": [
    "# Morphology\n",
    "https://spacy.io/usage/linguistic-features#morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "token = doc[0]  # 'I'\n",
    "print(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\n",
    "print(token.morph.get(\"PronType\"))  # ['Prs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62417153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5703ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c09da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a0821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddb1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca38ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4febe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "170975a1",
   "metadata": {},
   "source": [
    "# Constituency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde5927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79e0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47151e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd32496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f3d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb6d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc60eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4c91c3",
   "metadata": {},
   "source": [
    "# Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadd6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e02546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2eb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3e222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2927c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2bd493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a39d2f89",
   "metadata": {},
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c4b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f2521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88601695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6c99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f66e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a3dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0237d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e4d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332f747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c307d87",
   "metadata": {},
   "source": [
    "# Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0728e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330c287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e34f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8d8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ecdb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c76f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660fed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c01c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028de5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10a95b83",
   "metadata": {},
   "source": [
    "# Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61bf466",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcda4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our case\n",
    "doc = nlp(data)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start, ent.end, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities_spacy(doc):\n",
    "    \"\"\"Fetch named entities from data with specified pipeline and optional keyword attributes.\n",
    "    \n",
    "    [Args]:\n",
    "            doc[class object]: The doc object of the data of interest after application of a spaCy pipeline.\n",
    "            \n",
    "    [Returns]:\n",
    "            [dict]: Dictionary containing the identified named entities by name linked to a list of lists\n",
    "                    containing start, end and label for each individual instance.\"\"\"\n",
    "\n",
    "    # define defaultdict to store the named entities\n",
    "    named_entities = defaultdict(list)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        # add the entities label, start_char and end_char to the dictionary\n",
    "        named_entities[ent.text].append([ent.start, ent.end, ent.label_])\n",
    "\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce7d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_ent = named_entities_spacy(\"../data/Original/iued_test_original.txt\", \"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(named_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0462e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31568bfc",
   "metadata": {},
   "source": [
    "# Unique Tokens or smthg :)\n",
    "\n",
    "\n",
    "Nicht das was named entities eigentlich will...\n",
    "Für einzelne Tokens: [Matcher](https://spacy.io/usage/rule-based-matching) \\\n",
    "Für ganze Sätze: [Phrasematcher](https://spacy.io/usage/rule-based-matching#phrasematcher) \\\n",
    "Ich schätze für den Moment sind wir nur an einzelnen Tokens interessiert? Oder an allen einzigartigen Token im ganzen Text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b43ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from collections import defaultdict # \n",
    "\n",
    "# this works on the same example as above\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# initialize the matcher, the vocab has to be the same as for the text\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# idealy the user specifies what he wants to search and what attribute to assign?\n",
    "terms = [[{\"LOWER\":\"audi\"}], [{\"LOWER\":\"improvements\"}], [{\"LOWER\":\"parking\"}]]\n",
    "\n",
    "# also supports regular expressions:\n",
    "terms += [[{\"TEXT\":{\"REGEX\":\"^[Ii](\\.?|f)$\"}}]] # search for I, i, If, if\n",
    "\n",
    "print(\"Query: {}\".format(terms))\n",
    "\n",
    "# add the terms to look for to the mathcer\n",
    "matcher.add(\"Query\", terms)\n",
    "\n",
    "# load the data into doc\n",
    "doc = nlp(data)\n",
    "# run the matcher on the text in doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# get the indices (would correspond to corpus position from cwb?)\n",
    "indices = [[start,end] for _,start,end in matches]\n",
    "\n",
    "print(indices)\n",
    "\n",
    "dict_out = defaultdict(list) # default dict initializes the value of a new key that is added with an empty list\n",
    "                             # which we can then append to\n",
    "\n",
    "# put the found indices to access the searched terms in a dictionary where they are available via said terms\n",
    "# We have to go through all the found entities to confirm what term they correspond to...\n",
    "# For large texts where we have many hits faster to split the query beforehand? Maybe parallel searching?\n",
    "for index_pair in indices:\n",
    "    dict_out[\"{}\".format(doc[index_pair[0]:index_pair[1]])].append(index_pair)\n",
    "\n",
    "# display the output\n",
    "for key in dict_out:\n",
    "    print(\"{} found at location {}.\".format(key, dict_out[key]))\n",
    "\n",
    "#for index in indices:\n",
    "#    print('{} found at index location {}'.format(doc[index], index))\n",
    "\n",
    "#print(matches)\n",
    "\n",
    "#for match_id, start, end in matches:\n",
    "#    string_id = nlp.vocab.strings[match_id] #Get string representation\n",
    "#    span = doc[start:end] # matched span\n",
    "#    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also search for words of certain length or above/below certain lengths\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [[{\"LENGTH\":{\"==\":10}}]]#, [{\"LENGTH\":{\"<=\":1}}], [{\"LENGTH\":{\">=\":12}}]]\n",
    "\n",
    "matcher.add(\"Query\", pattern)\n",
    "matches = matcher(doc)\n",
    "\n",
    "indices = [[start,end] for _,start,end in matches]\n",
    "\n",
    "dict_out = defaultdict(list)\n",
    "\n",
    "for index_pair in indices:\n",
    "    dict_out[\"{}\".format(doc[index_pair[0]:index_pair[1]])].append(index_pair)\n",
    "\n",
    "for key in dict_out:\n",
    "    print(\"{} found at location {}.\".format(key, dict_out[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for token pattern\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# seach for the different types of cars with a \"wildcard token pattern\" leaving the last token empty\n",
    "pattern = [[{\"ORTH\":\"Audi\"}, {\"ORTH\": \"A\"}, {}]] \n",
    "\n",
    "matcher.add(\"Query\", pattern)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "indices = [[start,end] for _,start,end in matches]\n",
    "\n",
    "for _, start, end in matches:\n",
    "    indices.append([start, end])\n",
    "\n",
    "dict_out = defaultdict(list)\n",
    "\n",
    "for index_pair in indices:\n",
    "    dict_out[\"{}\".format(doc[index_pair[0]:index_pair[1]])].append(index_pair)\n",
    "\n",
    "for key in dict_out:\n",
    "    print(\"{} found at location {}.\".format(key, dict_out[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it in a function\n",
    "\n",
    "def search_text(query, nlp, doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    matcher.add(\"Query\", query)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    indices = [[start,end] for _,start,end in matches]\n",
    "\n",
    "    dict_out = defaultdict(list)\n",
    "\n",
    "    for index_pair in indices:\n",
    "        dict_out[\"{}\".format(doc[index_pair[0]:index_pair[1]])].append(index_pair)\n",
    "\n",
    "    #for key in dict_out:\n",
    "    #    print(\"{} found at location {}.\".format(key, dict_out[key]))\n",
    "    return dict_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [[{\"ORTH\":\"Audi\"}, {\"ORTH\": \"A\"}, {}]] \n",
    "\n",
    "test1 = search_text(query, nlp, doc)\n",
    "\n",
    "print(test1)\n",
    "\n",
    "print('*'*50)\n",
    "\n",
    "# can just add different queries together\n",
    "query += terms\n",
    "\n",
    "test2 = search_text(query, nlp, doc)\n",
    "\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c897f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(doc):\n",
    "    \"\"\"Get number of unique words in doc\"\"\"\n",
    "    \n",
    "    seen = set()\n",
    "    for token in doc:\n",
    "        if token.text not in seen:\n",
    "            seen.add(token.text)\n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_token(query, nlp, doc):\n",
    "    \"\"\"search text for specific token and return all the found locations in dict.\"\"\"\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    matcher.add(\"Query\", query)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    indices = [[start,end] for _,start,end in matches]\n",
    "\n",
    "    dict_out = defaultdict(list)\n",
    "\n",
    "    dict_out[\"{}\".format(doc[indices[0][0]:indices[0][1]])].append(indices)\n",
    "\n",
    "    #for key in dict_out:\n",
    "    #    print(\"{} found at location {}.\".format(key, dict_out[key]))\n",
    "    return dict_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_tokens(nlp, doc):\n",
    "    \"\"\"Get locations for all umique words in doc into a dictionary, case sensitive\"\"\"\n",
    "    \n",
    "    unique_tokens = None\n",
    "\n",
    "    # get the number of unique tokens in text, so we don't index twice\n",
    "    unique = get_unique(doc)\n",
    "\n",
    "    for token in unique:\n",
    "        \n",
    "        if unique_tokens:\n",
    "            # if the dictionary is not empty:\n",
    "            if token in unique_tokens:\n",
    "                # if the token is already in the dictionary:\n",
    "                pass\n",
    "            elif token not in unique_tokens:\n",
    "                # if the token is not already in there add it:\n",
    "                unique_tokens.update(search_token([[{\"ORTH\":\"{}\".format(token)}]], nlp, doc))\n",
    "        else:\n",
    "            # if the dictionary hasn't been initialized do so with first token\n",
    "            unique_tokens = search_token([[{\"ORTH\":\"{}\".format(token)}]], nlp, doc)\n",
    "        \n",
    "        if len(unique_tokens) == unique:\n",
    "            # if we have passed each unique word already there is no need to continue\n",
    "            break\n",
    "\n",
    "    return unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tok = unique_tokens(nlp, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in named_ent:\n",
    "    print(\"{}: {}\".format(key, named_ent[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32668878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(named_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(named_ent[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d0858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
