{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentencizer\n",
    "Needed here as everything has to be a sentence anyways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constituency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "from collections import defaultdict\n",
    "\n",
    "# get the sentence\n",
    "sentence = Sentence(\"I love Berlin .\")\n",
    "# load the NER model\n",
    "tagger = SequenceTagger.load(\"ner\")\n",
    "# apply model to sentence\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence)\n",
    "print(\"The following NER tags are found:\")\n",
    "\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans(\"ner\"):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Original/iued_test_original.txt\"\n",
    "\n",
    "with open(path, \"r\") as file:\n",
    "    data = file.read().replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SegtokSentenceSplitter.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently Flair can only work on individual sentences, it does however provide a function to split text into sentences. For the test-data it seems to be unable to do so, turning it into one long \"sentence\" instead.  \n",
    "  \n",
    "-> The problem seems to be related with the spaces before the punctuation in the text. This seems to disable the splitter from recognizing the sentences as separate units.  \n",
    "  \n",
    "If we have this text split into sentences from a different programm we could also just use those sentences directly.  \n",
    "  \n",
    "-> leads to indexing problems as the char positions are then labeled according to the individual sentences, disregarding the original position in the whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the splitter\n",
    "splitter = SegtokSentenceSplitter()\n",
    "\n",
    "# try to split the test-data as it is provided\n",
    "sentences = splitter.split(data)\n",
    "\n",
    "# take a look at one long sentence\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the data to not contain spaces before punctuation -> enable flairs sentencizer.\n",
    "data1 = data.replace(\" . \", \". \")\n",
    "\n",
    "# separate into sentences and discard any empty entries that may arise\n",
    "sentences1 = [sentence for sentence in splitter.split(data1) if len(sentence) != 0]\n",
    "\n",
    "# show new split text\n",
    "for sentence in sentences1:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences1[-1].to_original_text())\n",
    "print(len(sentences1[-1].to_original_text()))\n",
    "print(len(sentences1[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger1 = SequenceTagger.load(\"ner-fast\")\n",
    "\n",
    "tagger.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_ent = defaultdict(list)\n",
    "\n",
    "# iterate through the sentences\n",
    "for sentence in sentences:\n",
    "    # for found entities in the sentence\n",
    "    for entity in sentence.get_spans():\n",
    "        # extract the text, label and position of start and end char\n",
    "        named_ent[\n",
    "            \"Text: {} |Label: {}\".format(entity.text, str(entity.labels[0]).split()[0])\n",
    "        ].append([int(entity.start_pos), int(entity.end_pos)])\n",
    "\n",
    "print(named_ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair seems to be quite slow compared to spacy and stanza, even while using the supposedly fast 'ner-fast' model. This is due to the fact that we are working on one really long sentence. If the sentence splitting would acutally work the progress should be much faster as displayed in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.predict(sentences1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "significantly faster like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_ent1 = defaultdict(list)\n",
    "idx = 0\n",
    "for sentence in sentences1:\n",
    "    for entity in sentence.get_spans():\n",
    "        named_ent1[\n",
    "            \"Text: {} |Label: {}\".format(entity.text, str(entity.labels[0]).split()[0])\n",
    "        ].append([int(entity.start_pos) + idx, int(entity.end_pos) + idx])\n",
    "    idx += len(sentence.to_original_text()) + 2\n",
    "\n",
    "print(named_ent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the problem of only indexing the individual sentences remains! Do we want to keep track of the index manually as above? If so, we need to be sure that the input data will always have a space before punctuation. We could also strip these before and just work on the \"reduced\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in sentences:\n",
    "# get the spans for each entity\n",
    "# for entity in sentence.get_spans():\n",
    "# print the text associated\n",
    "# print(entity.text)\n",
    "# grab the position in the corpus, subtract one to account for\n",
    "# the indexing starting at 1 in flair (cwb starts at 0)\n",
    "# pos = [entity.start_pos, entity.end_pos]\n",
    "\n",
    "# print(pos)\n",
    "# print(str(entity.labels[0]).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Indices start at 1 in flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities_flair(data):\n",
    "\n",
    "    splitter = SegtokSentenceSplitter()\n",
    "\n",
    "    sentences = [sentence for sentence in splitter.split(data) if len(sentence) != 0]\n",
    "\n",
    "    # load the NER model\n",
    "    tagger = SequenceTagger.load(\"ner\")\n",
    "    # apply model to sentence\n",
    "    tagger.predict(sentences, mini_batch_size=32)\n",
    "    # set up dictionary\n",
    "    named_entities = defaultdict(list)\n",
    "    idx = 0\n",
    "    # extract NER data from flair\n",
    "    for sentence in sentences:\n",
    "        for entity in sentence.get_spans():\n",
    "            named_entities[\n",
    "                \"Text: {} |Label: {}\".format(\n",
    "                    entity.text, str(entity.labels[0]).split()[0]\n",
    "                )\n",
    "            ].append([int(entity.start_pos + idx), int(entity.end_pos + idx)])\n",
    "        idx += len(sentence.to_original_text()) + 2\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use already reduced data here\n",
    "named_ent = named_entities_flair(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(named_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to original data, only works if every sentence ends in \" .\" for the original.\n",
    "for idx in named_ent[\"Text: Audi A |Label: MISC\"]:\n",
    "    print(data[idx[0] - 20 : idx[1] + 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with pre-separated sentences from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentencize with spacy\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "doc = nlp(data)\n",
    "\n",
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentences.append(str(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the sentences from spacy objects to flair objects\n",
    "new_sent = []\n",
    "for sentence in sentences:\n",
    "    new_sent.append(Sentence(sentence))\n",
    "\n",
    "# check out the new sentences\n",
    "for sentence in new_sent:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the NER\n",
    "tagger.predict(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results\n",
    "temp = defaultdict(list)\n",
    "\n",
    "for sentence in new_sent:\n",
    "    for entity in sentence.get_spans():\n",
    "        temp[\n",
    "            \"Text: {} |Label: {}\".format(entity.text, str(entity.labels[0]).split()[0])\n",
    "        ].append([int(entity.start_pos), int(entity.end_pos)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the results\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be significantly faster than working on one long \"sentence\". We do however lose the continuous char indexing and only get the indices for the respective sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
