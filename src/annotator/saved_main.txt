
spacy main

if __name__ == "__main__":
    data = be.prepare_run.get_sample_text()
    # or read the main dict and activate
    mydict = be.prepare_run.load_input_dict("src/annotator/input")
    # take only the part of dict pertaining to spacy
    # filename needs to be moved to/taken from top level of dict
    # spacy_dict = mydict["spacy_dict"]
    # remove comment lines starting with "_"
    # for now, we are not using "components" as these are defined through the pre-
    # made models; for making your own model, they will need to be used
    # we will worry about this later
    spacy_dict = be.prepare_run.update_dict(mydict)
    # build pipe from config, apply it to data, write results to vrt
    # spacy_pipe(spacy_dict).pply_to(data).pass_results()
    # if we use "outname", this needs to be passed the full dict
    spacy_pipe(mydict).apply_to(data).pass_results()

    # this throws a warning that the senter may not work as intended, it seems to work
    # fine though
    # senter_config = {
    #     "filename": "Test1",
    #     "lang": "en",
    #     "text_type": "news",
    #     "processors": "tok2vec,tagger,attribute_ruler,lemmatizer",
    #     "pretrained": False,
    #     "set_device": False,
    #     "config": {},
    # # }

    # # spacy_pipe(senter_config).apply_to(data).begin_to_vrt()
    # # try to chunk the plenary text from example into pieces, annotate these and than reasemble to .vrt
    # # get chunked text
    data = be.chunk_sample_text("data/Original/plenary.vrt")

    # # start with basic config as above if we use the pretrained keyword it
    # # replaces the lang and text_type keys so we don't need to specifiy them

    # # some testing to check wheter spacy_pipe.pipe_multiple or spacy_pipe.get_multiple is faster
    # config = {
    #     "filename": "test_new",
    #     "processors": "tok2vec, tagger, parser,\
    #         attribute_ruler, lemmatizer, ner",
    #     "pretrained": "de_core_news_md",
    #     "set_device": False,
    #     "config": {"nlp.batch_size": 10},
    # }

    # config1 = {
    #     "filename": "test_class",
    #     "processors": "tok2vec, tagger, parser,\
    #         attribute_ruler, lemmatizer, ner",
    #     "pretrained": "de_core_news_md",
    #     "set_device": False,
    #     "config": {"nlp.batch_size": 10},
    # }

    # repeat = 50

    pipe = spacy_pipe(spacy_dict)
    pipe.pipe_multiple(data)

    # stmt = """pipe.pipe_multiple(data)"""

    # Time = timeit.repeat(
    #     stmt=stmt, number=1, repeat=repeat, timer=time.process_time, globals=globals()
    # )

    # check = []
    # with open("test_new_spacy.vrt", "r") as file:
    #     for line in file:
    #         if not line.startswith("!") and not line.startswith("<"):
    #             check.append(int(line.split()[0]))

    # for i, elem in enumerate(check):
    #     if i > 0:
    #         try:
    #             assert elem - check[i - 1] == 1
    #         except AssertionError:
    #             print(i, elem)
    # print("Asserted pipe indexing.")

    # pipe = spacy_pipe(config1)

    # stmt1 = """pipe.get_multiple(data)"""

    # Time1 = timeit.repeat(
    #     stmt=stmt1, number=1, repeat=repeat, timer=time.process_time, globals=globals()
    # )

    # check = []
    # with open("test_class_spacy.vrt", "r") as file:
    #     for line in file:
    #         if not line.startswith("!") and not line.startswith("<"):
    #             check.append(int(line.split()[0]))

    # for i, elem in enumerate(check):
    #     if i > 0:
    #         try:
    #             assert elem - check[i - 1] == 1
    #         except AssertionError:
    #             print(i, elem)
    # print("Asserted iterating indexing.")

    # print(
    #     "Using spacy.pipe: {:.2f}s | Iterating directly: {:.2f}s".format(
    #         min(Time), min(Time1)
    #     )
    # )

    # # check that output is indeed the same for both methods
    # with open("test_new_spacy.vrt", "r") as file:
    #     out1 = file.readlines()

    # with open("test_class_spacy.vrt", "r") as file:
    #     out = file.readlines()

    # i = 0
    # equal = True
    # for i, line in enumerate(out):
    #     if not line.startswith("!"):
    #         try:
    #             assert line == out1[i]
    #         except AssertionError:
    #             if i < 10:
    #                 print(line, out1[i])
    #                 equal = False
    #                 i += 1
    # print("Asserted equality")


# with open("out/test_spacy.vrt", "r") as file:
# for line in file:
# check if vrt file was written correctly
# lines with "!" are comments, <s> and </s> mark beginning and
# end of sentence, respectively
# if line != "<s>\n" and line != "</s>\n" and line.split()[0] != "!":
# try:
#    assert len(line.split()) == len(spacy_dict["processors"].split(","))
# except AssertionError:
#    print(line)


mstanza main

if __name__ == "__main__":
    dict = be.prepare_run.load_input_dict("./src/annotator/input")
    outfile = dict["output"]
    # take only the part of dict pertaining to stanza
    stanza_dict = dict["stanza_dict"]
    # to point to user-defined model directories
    # stanza does not accommodate fully at the moment
    mydict = mstanza_preprocess.fix_dict_path(stanza_dict)
    print(stanza_dict)
    print(mydict)
    # stanza does not care about the extra comment keys
    # but we remove them for subsequent processing just in case
    # now we need to select the processors and "activate" the sub-dictionaries
    mydict = be.prepare_run.update_dict(mydict)
    mydict = be.prepare_run.activate_procs(mydict, "stanza_")
    mytext = be.prepare_run.get_sample_text()
    # mytext = "This is an example. And here we go."
    # initialize instance of the class
    obj = mstanza_pipeline(mydict)
    obj.init_pipeline()
    out = obj.process_text(mytext)
    obj.postprocess(outfile)
    # For the output:
    # We need a module that transforms a generic dict into xml.


mflair main

if __name__ == "__main__":

    cfg = be.prepare_run.load_input_dict("src/annotator/input")
    data = "This is a sentence. This is another sentence, or is it?"
    flair_pipe(cfg).senter_spacy(data).apply_to().get_out()

    # check that the indexing is correct -> no skips or setbacks

    check = []
    with open("Test_flair.vrt", "r") as file:
        for line in file:
            if not line.startswith("!") and not line.startswith("<"):
                check.append(int(line.split()[0]))

    for i, elem in enumerate(check):
        if i > 0:
            try:
                assert elem - check[i - 1] == 1
            except AssertionError:
                print(i, elem)

    data1 = be.chunk_sample_text("data/Original/plenary.vrt")

    cfgman = {"input": "testplenary", "output": "out/test_chunk_flair", 
    "flair_dict":{"lang": "de", "job": ["ner", "pos"]}}

    flair_pipe(cfgman).get_multiple(data1)

    # index check -> gibt noch ein paar Probleme, bin dran
    # // scheint gefixed zu sein, schaue am Donnerstag nochmal drauf
    check = []
    with open("testplenary_flair.vrt", "r") as file:
        for line in file:
            if not line.startswith("!") and not line.startswith("<"):
                check.append(int(line.split()[0]))

    for i, elem in enumerate(check):
        if i > 0:
            try:
                assert elem - check[i - 1] == 1
            except AssertionError:
                print(i, elem)

to_xml main

if __name__ == "__main__":
    dict = be.prepare_run.load_input_dict("./src/annotator/input")
    outfile = dict["output"]
    # take only the part of dict pertaining to stanza
    stanza_dict = dict["stanza_dict"]
    # to point to user-defined model directories
    # stanza does not accommodate fully at the moment
    mydict = ms.mstanza_preprocess.fix_dict_path(stanza_dict)
    # stanza does not care about the extra comment keys
    # but we remove them for subsequent processing just in case
    # now we need to select the processors and "activate" the sub-dictionaries
    mydict = be.prepare_run.update_dict(mydict)
    mydict = be.prepare_run.activate_procs(mydict, "stanza_")
    mytext = be.prepare_run.get_sample_text()
    # mytext = "This is an example. And here we go."
    # initialize instance of the class
    obj = ms.mstanza_pipeline(mydict)
    obj.init_pipeline()
    out = obj.process_text(mytext)
    # obj.postprocess(outfile)

    # get the output as a List of Lists of Dicts
    dicts = out.to_dict()

    # start the xml
    raw_xml = Element("doc")

    # iterate the sentences
    for i, elem in enumerate(dicts, 1):
        raw_xml.append(list_to_xml("Sent", i, elem))

    # convert and decode the byte str
    out_xml = tostring(raw_xml, "unicode")

    # beautify the xml
    parsed = mini.parseString(out_xml)
    with open("test.xml", "w") as file:
        file.write(parsed.toprettyxml())
    # For the output:
    # We need a module that transforms a generic dict into xml.

get_test_xml
import to_xml as txml
import base as be
import mstanza as ma


mydict = {
    "lang": "en",
    "dir": "./test/models/",
    "processors": "tokenize,pos,lemma",
}
obj = ma.mstanza_pipeline(mydict)
obj.init_pipeline()

with open("./test/test_files/example_en.txt") as f:
    text = f.read().replace("\n", "")

doc = obj.process_text(text)

data = doc.to_dict()

print(type(data))

raw_xml = txml.Element("doc")

sents = [txml.list_to_xml("Sent", i, elem) for i, elem in enumerate(data, 1)]

for sent in sents:
    raw_xml.append(sent)

# for i, elem in enumerate(data, 1):
#     raw_xml.append(txml.list_to_xml("Sent", i, elem))

raw_xml = txml.to_string(raw_xml)

xml = txml.beautify(raw_xml)

with open("example_en.xml", "w") as file:
    file.write(xml)