{
    "input": "input.txt",
    "output": "out/output",
    "tool": "spacy",
    "cwb_dict": {
        "_corpus_name_comment": "Name of corpus to be annotated.",
        "corpus_name": "test",
        "_corpus_dir_comment": "Directory containing the corpus.",
        "corpus_dir": "/home/jovyan/shared/corpora/",
        "_registry_dir_comment": "Name of registry directory for cwb.",
        "registry_dir": "/home/jovyan/shared/registry/"
    },
    "stanza_dict": {
        "_comment": "Main stanza dictionary; processor-specific options are set with keys {processor_name}_{argument_name}",
        "_lang_comment": "Language code for the language to build the Pipeline in",
        "lang": "en",
        "_dir_comment": "directory where models are stored - this maynot work for pipeline",
        "dir": "./src/annotator/test/models/",  
        "_package_comment": "using the default model, for others see: https://stanfordnlp.github.io/stanza/models.html",
        "package": "default",
        "_processors_comment": "Comma-separated list of processors to use, can also be given as a dictionary: {'tokenize': 'ewt', 'pos': 'ewt'}",
        "processors": "tokenize,pos,lemma",  
        "_logging_level_comment": "DEBUG, INFO, WARN, ERROR, CRITICAL, FATAL; FATAL has least amount of log info printed",
        "logging_level": "INFO",  
        "_verbose_comment": "True corresponds to INFO, False corresponds to ERROR",
        "verbose": true,
        "_use_GPU_comment:":"use GPU if available, False forces CPU only",
        "use_GPU": false,
        "stanza_tokenize": {  
            "_tokenize_comment": "Tokenizes the text and performs sentence segmentation, dependency: -",
            "_tokenize_model_path_comment": "set model path only for custom models",
            "_tokenize_batch_size_comment": "Maximum number of paragraphs to process as minibatch for efficient processing",
            "tokenize_batch_size": 32,
            "_tokenize_pretokenized_comment": "Text is already tokenized by white space and sentence split by newline, no usage of a model",
            "tokenize_no_ssplit": false
        },
        "stanza_mwt": {
            "_mwt_comment": "Expands multi-word tokens (MWT) predicted by the TokenizeProcessor, this is only applicable to some languages, dependency: - 'tokenize'",
            "_mwt_model_path_comment": "set model path only for custom models",
            "_mwt_batch_size_comment": " Specifies maximum number of workds to process as a minibatch",
            "mwt_batch_size": 50
        },
        "stanza_pos": {
            "_pos_comment": "Labels tokens with their universal POS (UPOS) tags, treebank-specific POS (XPOS) tags, and universal morphological features (UFeats), dependency: - 'tokenize, mwt'.",
            "_pos_model_path_comment": "set model path only for custom models",
            "_pos_pretrain_path_comment": "set model path only for custom models",
            "_pos_batch_size_comment": "this specifies the maximum number of words to process as  minibatch for efficient processing. Default: 5000",
            "pos_batch_size": 5000
        },
        "stanza_lemma": {
            "_lemma_comment": "Generates the word lemmas for all words in the Document, dependency: - 'tokenize, mwt, pos'",
            "_lemma_model_path_comment": "set model path only for custom models",
            "_lemma_use_identity_comment": "Identity lemmatizer is used instead of statistical lemmatizer",
            "lemma_use_identity": false,
            "_lemma_batch_size_comment": "Maximum number of words to batch for efficient processing. Default: 50",
            "lemma_batch_size": 50,
            "_lemma_ensemble_dict_comment": "Lemmatizer will ensemble a seq2seq model with output from dictionary-based lemmatizer, improvement for many languages",
            "lemma_ensemble_dict": true,
            "_lemma_dict_only_comment": "Dictionary-based lemmatizer only",
            "lemma_dict_only": false,
            "_lemma_edit_comment": "Use an edit classifier in addition to seq2seq to make long sentence predictions more stable",
            "lemma_edit": true,
            "_lemma_beam_size_comment": "Control beam size during decoding in seq2seq",
            "lemma_beam_size": 1,
            "_lemma_max_dec_len_comment": "Control maximum decoding character length in seq2seq, decoder will stop if this length is achieved and end-of-sequence character still not seen",
            "lemma_max_dec_len": 50
        },
        "stanza_depparse" : {
            "_depparse_comment": "Provides an accurate syntactic dependency parsing analysis, dependency: - 'tokenize, mwt, pos, lemma'",
            "_depparse_model_path_comment": "set model path only for custom models",
            "_depparse_batch_size_comment": "Maximum number of words to process as minibatch, may require large amounts of RAM; should be set larger than the number of workds in the longest sentence in input document, or may result in unexpected behaviors",
            "depparse_batch_size": 5000,
            "_depparse_pretagged_comment": "Assumes document is tokenized and pretagged, only run dependency parsing",
            "depparse_pretagged": false
        },
        "stanza_ner": {
            "_ner_comment": "Recognize named entities for all token spans in the corpus, dependency: - 'tokenize, mwt'",
            "_ner_model_path_comment": "set model path only for custom models",
            "_ner_batch_size_comment": "Maximum number of sentences to process as minibatch, may require large amounts of RAM",
            "ner_batch_size": 32
        },
        "stanza_sentiment": {
            "_sentiment_comment": "Assign per-sentence sentiment scores, dependency: - 'tokenize, mwt'",
            "_sentiment_model_path_comment": "set model path only for custom models",
            "_sentiment_pretrain_path_comment": "which set of pretrained word vectors to use, closely related to used model",
            "_batch_size_comment": "Run everything at once (None) or if set to integer, processing is broken into chunks of that size",
            "batch_size": null
        },
        "stanza_constituency": {
            "_constituency_comment": "Parse each sentence in a document using a phrase structure parser, dependency: - 'tokenize, mwt, pos'",
            "_constituency_model_path_comment": "set model path only for custom models",
            "_constituency_pretrain_path_comment": "which set of pretrained word vectors to use, closely related to used model"
        }
    },
    "spacy_dict": {
        "_comment": "Main spacy dictionary, set the processors for the pipeline, the base model to use and more config.",
        "_model_comment": "Pretrained/trained model to load and use components from. IF model is not installed as package a path to the directory containing the models config.cfg file can be provided.",
        "model": "en_core_web_md",
        "_model_path_example": "/home/christian/anaconda3/envs/cwbproj/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-3.1.0 -> Path to directory containing config.cfg for en_core_web_sm installed as package in conda environment.",
        "_lang_comment": "Language of the text to be annotated to chose model if no specific model is given.",
        "lang": "en",
        "_text_type_comment": "Type of text to be processed, current options: news, biomed for de and news for en.",
        "text_type": "news",
        "_processors_comment": "List of processors to be used in the pipeline. Sepparated by ',' may contain whitespaces.",
        "processors": "tok2vec, senter, tagger, parser, attribute_ruler, lemmatizer, ner",
        "_pretrained_comment": "Use a complete pretrained model - initialization of model with information from raw text instead of randomly.",
        "pretrained": false,
        "_set_device_comment": "Request job to be run on CPU only (require_CPU), on GPU if available (prefer_GPU), only on GPU (require_GPU, will fail if no GPU), default by false.",
        "set_device": false,
        "_exclude_comment": "Components from pipeline that are specifically not to be loaded upon initialization. If none are given all components will be loaded, only those specified in preocessors will be run.",
        "exclude": false,
        "config": {
            "_nlp.batch_size_comment": "Default batch size for pipe.",
            "nlp.batch_size": 256,
            "_components":{
                "tokenizer": {
                    "_tokenizer_comment": "Segment Text, create Doc object with discovered segment bounderies. Reads punctuation and special case rules from Language.Defaults.",
                    "_rules_comment": "Exceptions and special-cases for the tokenizer. [Dict[str, List[Dict[int, str]]] e.g. [{ORTH: 'do'}, {ORTH: 'n't', NORM: 'not'}]"
                },
                "attribute_ruler": {
                    "_attribute_ruler_comment": "Pipeline component for rule-based token attribute assignment. Trainable: No.",
                    "_validate_comment": "Wheter patterns should be validated, Defaults to false.",
                    "validate": false
                },
                "parser": {
                    "_dependency_parser_comment": "Pipeline component for syntactic dependency parsing, coresponds to DependencyParser. Trainable: Yes.",
                    "_moves_comment": "List of transition names. Inferred from data if not provided",
                    "moves": null,
                    "_model_comment": "Model powering the pipeline component and parameters for model.",
                    "_min_action_frequency_comment": "Sets the minimum frequency of labelled actions to retain, affects label accuracy and possibly attachment structure.",
                    "_learn_tokens_comment": "Decides whether to learn to merge subtokens split relative to gold standard. Default to false, Experimental.",
                    "learn_tokens": false,
                    "_update_with_oracle_cut_size_comment": "During training: Cut long sequences into shorter segments by creating intermediate standards based on gold-standard history. Default 100.",
                    "update_with_oracle_cut_size": 100
                },
                "entity_linker": {
                    "_entity_linker": "Pipeline component for named entity linking and disambiguation. Trainable: Yes. Requires: KnowledgeBase (KB).",
                    "_model_comment": "Model powering pipeline component.",
                    "_entity_vector_length": "Size of encoding vectors in KB",
                    "_labels_discard_comment": "NER labels that will automatically get a 'NIL' prediction, Default []",
                    "labels_discard": null,
                    "_n_sents_comment": "Number of neighbouring sentences to take into account, Default 0.",
                    "n_sents": 0,
                    "_incl_prior_comment": "Whether or not to include prior probabilities from the KB in the model.",
                    "_incl_context_comment": "Whether or not to include the local context in the model.",
                    "_overwrite_comment": "Whether existing annotation is overwritten. Defaults to True.",
                    "overwrite": true
                },
                "entity_ruler": {
                    "_entity_ruler_comment": "Add spans to entities using token based rules or exact phrase matches. Trainable: No.",
                    "_phrase_matcher_attr_comments": "Optional attribute name match on for the internal PhraseMatcher, e.g. LOWER to match on the lowercase token text. Defaults to None.",
                    "phrase_matcher_attr" : null,
                    "_validate_comment": "Wheter patterns should be validated (passed to Matcher and PhraseMatcher), Default False.",
                    "validate": false,
                    "_overwrite_ents_comment": "If existing entities are present, e.g. entities added by the model, overwrite them by matches if necessary. Defaults to False.",
                    "overwrite_ents": false,
                    "_ent_id_sep": "Separator used internally for entity IDs. Defaults to '||'.",
                    "ent_id_sep": "||",
                    "_patterns_comment": "Optional patterns to load in on initialization. Shape List[Dict[str, Union[str, List[dict]]]] e.g. [{'label': 'ORG', 'pattern': 'Apple'},{'label': 'GPE', 'pattern': [{'lower': 'san'}, {'lower': 'francisco'}]}]" 
                },
                "ner": {
                    "_ner_comment": "Pipeline component for transition-based named entity recognition, corresponds to EntityRecognizer. Idejtifies non-overlapping labelled spans of tokens. Trainable: Yes.",
                    "_model_comment": "Model powering pipeline component.",
                    "_moves_comment": "A list of transition names. Inferred from the data if set to None, which is the default.",
                    "moves": null,
                    "_update_with_oracle_cut_size_comment": "During training: Cut long sequences into shorter segments by creating intermediate standards based on gold-standard history. Default 100.",
                    "update_with_oracle_cut_size": 100,
                    "_incorrect_spans_key": "Identifies spans that are known to be incorrect entity annotations. The incorrect entity annotations can be stored in the span group in Doc.spans, under this key. Defaults to None.",
                    "incorrect_spans_key": null
                },
                "lemmatizer": {
                    "_lemmatizer_comment": "Pipeline component for assignment of base forms to tokens based on POS tags or lookup tables. Trainable: No. Requires: spacy-lookups-data or POS tags",
                    "_mode_comment": "Lemmatizer mode, 'lookup' or 'rule'. Lookup needs lookup tables. rule needs coarse-grained POS.",
                    "mode": "rule",
                    "_overwrite_comment": "Wheter to overwrite existing lemmas. Default False",
                    "overwrite": false,
                    "_scorer_comment": "Scoring method, Defaults to Scorer.score_token_attr."
                },
                "morphologizer": {
                    "_morphologizer_comment": "Pipeline component to predict morphological features and coarse-grained UPOS tags. Trainable: Yes.",
                    "_model_comment": "Model to use, defaults to Tagger.",
                    "_overwrite_comment": "Whether the values of existing features are overwritten. Defaults to True.",
                    "overwrite": true,
                    "_extend_comments": "Whether existing feature types (whose values may or may not be overwritten depending on overwrite) are preserved. Defaults to False.",
                    "extend": false,
                    "_scorer_comment": "Scoring method, Defaults to Scorer.score_token_attr. for attributes 'pos' and 'morph' and Scorer.score_token_attr_per_feat for attribute 'morph'."
                },
                "senter": {
                    "_senter_comment": "Pipeline component for sentence segmentation. Trainable: Yes.",
                    "_model_comment": "Model powering pipeline component",
                    "_overwrite_comment": "Whether existing annotation is overwritten. Defaults to False.",
                    "overwrite": false,
                    "_scorer_comment": "Scoring method, Defaults to Scorer.score_spans for attribute 'sents'."
                },
                "sentencizer": {
                    "_sentencizer_comment": "Simple pipeline component for custom sentence boundary detection logic without dependency parse.",
                    "_punct_chars_comment": "Optional custom list of punctuation characters that mark sentence ends. See below for defaults. List[str]",
                    "_overwrite_comment": "Whether existing annotation is overwritten. Defaults to False.",
                    "overwrite": false,
                    "_scorer_comment": "Scoring method, Defaults to Scorer.score_spans for attribute 'sents'."
                },
                "spancat": {
                    "_spancat_comment": "Pipeline component for labeling potentially overlapping spans of text. Trainable: Yes.",
                    "_model_comment": "Model instance given as list of documents and indices representing candidate span offsets. Predicts probability for each category for each span.",
                    "_suggester_comment": "Function that suggests spans, returns spans as ragged array of two integer columns for start and end positions.",
                    "_span_key": "Key of Doc.spans dict to save spans under. Looks for spans on reference document under same key during initilization and trianing. Defaul to 'spans'.",
                    "span_key": "spans",
                    "_threshhold_comment": "Minimum probability to consider a prediciton positive. Defaults to 0.5.",
                    "threshhold": 0.5,
                    "_max_positive_comment": "Maximum number of labels to consider positive per span. Defaults to None, indicating no limit.",
                    "max_positive": null
                },
                "tagger": {
                    "_tagger_comment": "Pipeline component for POS tagging. Trainable: Yes.",
                    "_model_comment": "Model instance predicting tag probabilities. Output vectors should match number  of tags in size and be normalized as probabilities. Defaults to Tagger.",
                    "_overwrite_comment": "Whether existing annotation is overwritten. Defaults to False.",
                    "overwrite": false,
                    "_scorer_comment": "Scoring method, Defaults to Scorer.score_token_attr. for attribute 'tag'.",
                    "_neg_prefix_comment": "The prefix used to specify incorrect tags while training. The tagger will learn not to predict exactly this tag. Defaults to !.",
                    "neg_prefix": "!"
                },
                "textcat": {
                    "_textcat_comment": "Pipeline component for text classification. predicts categories over a whole document. Use for one specific true label per document. For multi-level classification use textcat_multilabel.",
                    "_model_comment": "Thinc Model powering pipeline component",
                    "_threshold_comment": "Cutoff to consider a prediction “positive”, relevant when printing accuracy results.",
                    "_scorer_comment": "Scoring method, Defaults to Scorer.score_cats for attribute 'cats'."
                },
                "textcat_multilabel": {
                    "_textcat_multilevel_comment": "Pipeline component for multi-level classification of tokens into categories.",
                    "_model_comment": "Thinc Model powering pipeline component",
                    "_threshold_comment": "Cutoff to consider a prediction “positive”, relevant when printing accuracy results.",
                    "_scorer_comment": "Scoring method, Defaults to Scorer.score_cats for attribute 'cats'."
                },
                "tok2vec": {
                    "_tok2vec_comment": "Token-to-vectore model that sets its output to the Doc.tensor attribute.This is mostly useful to share a single subnetwork between multiple components.",
                    "_model_comment": "Model to use, Defaults to HashEmbedCNN."
                },
                "transformer" : {
                    "_transformer_comment": "Pipeline component for multi-task learning with transformer models.",
                    "_set_extra_annotations_comment": "Function that takes a batch of Doc objects and transformer outputs and stores the annotations on the Doc. The Doc._.trf_data attribute is set prior to calling the callback. By default, no additional annotations are set.",
                    "_max_batch_items": "Maximum size of padded batch. Defaults to 128*32"
                }
            }
        }
    },
    "flair_dict":{
        "_filename_comment":"Name of .vrt file to create.",
        "filename": "Test",
        "_lang_comment": "Language of the text, supports en and de.",
        "lang": "en",
        "_job_comment": "What to tag, options are 'ner', 'pos' or list of both '['pos', 'ner']'.",
        "job": ["ner", "pos"]
    }
}
