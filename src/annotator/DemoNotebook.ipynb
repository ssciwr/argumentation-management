{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook for annotator package\n",
    "## Scientific Software Center, I. S. Ulusoy, C. Delavier, Heidelberg University\n",
    "*January 2022*\n",
    "\n",
    "In the following the basic functionalities of the package are introduced. We will load basic text in English and German and annotate it using the available features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the modules of the annotator package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base as be\n",
    "import mspacy as msp\n",
    "import mstanza as sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is passed to the package using a dictionary (json-file). Later, this will be hidden in the user interface. For now, you will load the default dictionary which has all options pre-set to default values, and then replace the options that you specify for your desired processing.\n",
    "\n",
    "### Read the default dictionary - do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in input.json\n",
    "default_dict = be.prepare_run.load_input_dict(\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main input dictionary contains these important parameters:\n",
    "```\n",
    "    \"input\": \"./test/test_files/example_en.txt\",\n",
    "    \"tool\": \"spacy\",\n",
    "    \"corpus_name\": \"test\",\n",
    "    \"language\": \"en\",\n",
    "    \"document_type\": \"text\",\n",
    "    \"processing_option\": \"fast\",\n",
    "    \"processing_type\": \"tokenize\"\n",
    "```\n",
    "You can print these using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(default_dict[\"input\"])\n",
    "print(default_dict[\"tool\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a copy of the dictionary for your run\n",
    "mydict = default_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tell the programm that the data we want to annotate is stored in `example_en.txt` with path to the file `./test/test_files/` and that we want to use the tool `spacy` to annotate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available tools and options\n",
    "You need to specify which language you would like to process. It is also good to specify the processing options, like tokenization, part-of-speech, lemma, etc., although if not specified the package will select all that are available for the language. There is currently a restriction: The output that is generated in the end and passed to cwb can only contain one or several of these options: sentencize, tokenize, part-of-speech, lemma. All other options do get processed but are not written to the file yet. Here we need some more feedback on the format that is required for cwb.\n",
    "\n",
    "## SpaCy\n",
    "More information about SpaCy is found [here](https://spacy.io/). Generally, SpaCy supports [these languages](https://spacy.io/usage/models), but at the moment only English and German are available in the annotator package. We will add more languages based on your requests - so please get in touch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out which model is being used\n",
    "print(mydict[\"spacy_dict\"][\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which language has been selected\n",
    "print(mydict[\"language\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to change the model, if another one has been downloaded. At the moment, only `en_core_web_md` and `de_core_news_md` are available. We will add more upon request, so please get in touch!\n",
    "\n",
    "Now select the processors that you would like to use: For the default English pipeline, the available options are `tok2vec, senter, tagger, parser, attribute_ruler, lemmatizer, ner`, where the first two options are required for tokenization, and the other options are: [Dependency parser](https://spacy.io/api/dependencyparser), POS-tagging via the [attribute ruler](https://spacy.io/api/attributeruler), [lemma](https://spacy.io/api/lemmatizer), and [named-entity recognition](https://spacy.io/api/entityrecognizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which processors have been selected\n",
    "print(mydict[\"processing_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza\n",
    "The only other available tool at this moment is `stanza`. Looking at the default dictionary, we now set\n",
    "```\n",
    "mydict[\"tool\"] = \"stanza\"\n",
    "```\n",
    "For the processing with [stanza](https://stanfordnlp.github.io/stanza/), only tokenization, POS and lemma are implemented for the same reasons as above. German requires also `mwt`, but the multi-word expressions are not marked as such in the generated output file. For these, p-attributes will be included at a later stage.\n",
    "\n",
    "Please request additional models to the English and German ones that are currently installed. We will then add them to the Hub and you do not need to worry about downloads. For a list of available languages and models, see [here](https://stanfordnlp.github.io/stanza/available_models.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the keys for your specific run - do change\n",
    "\n",
    "You can now modify the keys to specify a different input file, output file, and selected tool as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a copy of the dictionary for your run\n",
    "mydict = default_dict\n",
    "# now you need to set your parameters\n",
    "# change the value of the key on the right hand of the \"=\"\n",
    "mydict[\"input\"] = \"./test/test_files/example_en.txt\"\n",
    "# change the value of the key on the right hand of the \"=\"\n",
    "mydict[\"tool\"] = \"spacy\"  # or \"stanza\" - so far, only spacy and stanza are implemented\n",
    "# specify the output directory of the vrt file\n",
    "mydict[\"advanced_options\"][\"output_dir\"] = \"./test/test_files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the \"\" around the keys - these are essential as the values are passed as string and should not be removed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the input - do not change\n",
    "The input is then validated to make sure all options have been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be.prepare_run.validate_input_dict(mydict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the input text to be processed as raw text - do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the raw text\n",
    "data = be.prepare_run.get_text(mydict[\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print the text as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also directly copy and paste text here - take care that it is surrounded by double quotes again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"This is my text. I like it better this way.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tool pipeline and process the text - do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get specific dict for spacy\n",
    "subdict = mydict[mydict[\"tool\"] + \"_dict\"]\n",
    "# load the pipeline using the selected options\n",
    "pipe = msp.spacy_pipe(subdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing this we only have to apply the pipeline to the data we read in earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pipeline to data\n",
    "annotated = pipe.apply_to(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the results of the pipeline  we can easily pass the results to a .vrt file using the output name defined in the .json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the annotated .vrt and pass to cwb\n",
    "annotated.pass_results(\"STR\", mydict, ret=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pipeline, applying it and passing the results can be done conveniently in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msp.spacy_pipe(subdict).apply_to(data).pass_results(\"STR\", mydict, ret=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the newly annotated corpus in cwb via cwb-ccc\n",
    "This needs to be adjusted on the jupyterjub as there are specific directories required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccc import Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = Corpora(\n",
    "    cqp_bin=\"/usr/local/bin/cqp\",\n",
    "    registry_path=\"/home/jovyan/shared/registry\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpora.activate(corpus_name=\"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.attributes_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the newly encoded corpus\n",
    "query = r'\"if\"'\n",
    "dump = corpus.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the query data frame\n",
    "dump.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = corpus.query(cqp_query=query, context=20, context_break=\"s\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "248d8c9983699a243ff8e6a68a4f537cc73e09b2b8d6acc84fd600ae9279441d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
