{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook for annotator package\n",
    "In the following the basic functionalities of the package are introduced. We will load basic text in english and german and annotate it using the available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import annotator.base as be\n",
    "# import annotator.mspacy as msp\n",
    "\n",
    "import base as be\n",
    "import mspacy as msp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The input\n",
    "The input for the package would normally be set up in an input.json file passed to the package.  There are two example .json files in this directory which we will use. Lets look at the contents of example_en.json. The first few parameters we encounter are:\n",
    "```\n",
    "    \"input\": \"example_en.txt\",\n",
    "    \"output\": \"output_en\",\n",
    "    \"tool\": \"spacy\",\n",
    "```\n",
    "These tell the programm that the data we want to annotate is stored in example_en.txt, that we want to output to a file which we can identify as output_en and that we want to use the tool spacy to annotate the data. There will be more tools to chose from, but for simplicity their configurations have been stripped for now. The spacy specific config is found in the `\"spacy_dict\"` section. Here we find the parameters we can tell spacy to enable it to annotate the data. The entries do usually come with a comment explaining what the parameters do. Lets look through the ones we set up in example_en.json:\n",
    "```\n",
    "    \"model\": false,\n",
    "```\n",
    "Here we can specify a model spacy should use to annotate the text if we want to. We leave it to false for now though.\n",
    "```\n",
    "    \"lang\": \"en\",\n",
    "```\n",
    "Here we specify that the language of the data we want to annotate is english. Since we didn't specify a model this information will be needed to chose one for us.\n",
    "```\n",
    "    \"text_type\": \"news\",\n",
    "```\n",
    "We specify what kind of text we want to annotate in order to chose an appropriate model for the task. This does currently only support \"news\" for english. The setup we chose here will lead to the usage of the model en_core_web_md.\n",
    "```\n",
    "    \"processors\": \"senter, tagger, parser, attribute_ruler, lemmatizer, ner\",\n",
    "```\n",
    "Here we specifiy the processors for the pipeline we will apply to our data. This will define what kind of annotations we get in the end.\n",
    "\n",
    "The remaining entries are not immediatly important for this example and are all set to their default values. Especially the `\"config\"` parameter and its contents are usually defined for a given model in it's config.cfg file and should not be tempered with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running based of the .json\n",
    "Let's now look at what the program would do with the supplied information from the .json input files. First we would read in the .json files to make them available as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in example_en.json\n",
    "dict_en = be.prepare_run.load_input_dict(\"example_en\")\n",
    "# print(dict_en)\n",
    "\n",
    "# read in example_de.json\n",
    "dict_de = be.prepare_run.load_input_dict(\"example_de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we habe access to the information from the .json files we can load in the data from the specified locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the english example text from example_en.txt\n",
    "data_en = be.prepare_run.get_text(dict_en[\"input\"])\n",
    "# print(data_en)\n",
    "\n",
    "# read in the german example text from example_de.txt\n",
    "data_de = be.prepare_run.get_text(dict_de[\"input\"])\n",
    "# print(data_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we would load the tool as specified by the .json. In this case we would load the spacy pipeline from the mspacy module. We are told what components we load and which model we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pipeline from the config\n",
    "pipe_en = msp.spacy_pipe(dict_en)\n",
    "\n",
    "pipe_de = msp.spacy_pipe(dict_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing this we only have to apply the pipeline to the data we read in earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pipeline to data\n",
    "annotated_en = pipe_en.apply_to(data_en)\n",
    "\n",
    "annotated_de = pipe_de.apply_to(data_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text has now been annotated. We can easily pass the results to a .vrt file using the output name defined in the .json. This would also directly encode the annotated results to cwb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the annotated .vrt and pass to cwb\n",
    "annotated_en.pass_results()\n",
    "\n",
    "annotated_de.pass_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pipeline, applying it and passing the results can be done conveniently in one line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example for english data\n",
    "msp.spacy_pipe(dict_en).apply_to(data_en).pass_results()\n",
    "\n",
    "# is equivalent to the above\n",
    "# pipe_en = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mspacy main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = be.prepare_run.get_sample_text()\n",
    "    # or read the main dict and activate\n",
    "    mydict = be.prepare_run.load_input_dict(\"src/annotator/input\")\n",
    "    # take only the part of dict pertaining to spacy\n",
    "    # filename needs to be moved to/taken from top level of dict\n",
    "    # spacy_dict = mydict[\"spacy_dict\"]\n",
    "    # remove comment lines starting with \"_\"\n",
    "    # for now, we are not using \"components\" as these are defined through the pre-\n",
    "    # made models; for making your own model, they will need to be used\n",
    "    # we will worry about this later\n",
    "    spacy_dict = be.prepare_run.update_dict(mydict)\n",
    "    # build pipe from config, apply it to data, write results to vrt\n",
    "    # spacy_pipe(spacy_dict).pply_to(data).pass_results()\n",
    "    # if we use \"outname\", this needs to be passed the full dict\n",
    "    spacy_pipe(mydict).apply_to(data).pass_results()\n",
    "\n",
    "    # this throws a warning that the senter may not work as intended, it seems to work\n",
    "    # fine though\n",
    "    # senter_config = {\n",
    "    #     \"filename\": \"Test1\",\n",
    "    #     \"lang\": \"en\",\n",
    "    #     \"text_type\": \"news\",\n",
    "    #     \"processors\": \"tok2vec,tagger,attribute_ruler,lemmatizer\",\n",
    "    #     \"pretrained\": False,\n",
    "    #     \"set_device\": False,\n",
    "    #     \"config\": {},\n",
    "    # # }\n",
    "\n",
    "    # # spacy_pipe(senter_config).apply_to(data).begin_to_vrt()\n",
    "    # # try to chunk the plenary text from example into pieces, annotate these and than reasemble to .vrt\n",
    "    # # get chunked text\n",
    "    data = be.chunk_sample_text(\"data/Original/plenary.vrt\")\n",
    "\n",
    "    # # start with basic config as above if we use the pretrained keyword it\n",
    "    # # replaces the lang and text_type keys so we don't need to specifiy them\n",
    "\n",
    "    # # some testing to check wheter spacy_pipe.pipe_multiple or spacy_pipe.get_multiple is faster\n",
    "    # config = {\n",
    "    #     \"filename\": \"test_new\",\n",
    "    #     \"processors\": \"tok2vec, tagger, parser,\\\n",
    "    #         attribute_ruler, lemmatizer, ner\",\n",
    "    #     \"pretrained\": \"de_core_news_md\",\n",
    "    #     \"set_device\": False,\n",
    "    #     \"config\": {\"nlp.batch_size\": 10},\n",
    "    # }\n",
    "\n",
    "    # config1 = {\n",
    "    #     \"filename\": \"test_class\",\n",
    "    #     \"processors\": \"tok2vec, tagger, parser,\\\n",
    "    #         attribute_ruler, lemmatizer, ner\",\n",
    "    #     \"pretrained\": \"de_core_news_md\",\n",
    "    #     \"set_device\": False,\n",
    "    #     \"config\": {\"nlp.batch_size\": 10},\n",
    "    # }\n",
    "\n",
    "    # repeat = 50\n",
    "\n",
    "    pipe = spacy_pipe(spacy_dict)\n",
    "    pipe.pipe_multiple(data)\n",
    "\n",
    "    # stmt = \"\"\"pipe.pipe_multiple(data)\"\"\"\n",
    "\n",
    "    # Time = timeit.repeat(\n",
    "    #     stmt=stmt, number=1, repeat=repeat, timer=time.process_time, globals=globals()\n",
    "    # )\n",
    "\n",
    "    # check = []\n",
    "    # with open(\"test_new_spacy.vrt\", \"r\") as file:\n",
    "    #     for line in file:\n",
    "    #         if not line.startswith(\"!\") and not line.startswith(\"<\"):\n",
    "    #             check.append(int(line.split()[0]))\n",
    "\n",
    "    # for i, elem in enumerate(check):\n",
    "    #     if i > 0:\n",
    "    #         try:\n",
    "    #             assert elem - check[i - 1] == 1\n",
    "    #         except AssertionError:\n",
    "    #             print(i, elem)\n",
    "    # print(\"Asserted pipe indexing.\")\n",
    "\n",
    "    # pipe = spacy_pipe(config1)\n",
    "\n",
    "    # stmt1 = \"\"\"pipe.get_multiple(data)\"\"\"\n",
    "\n",
    "    # Time1 = timeit.repeat(\n",
    "    #     stmt=stmt1, number=1, repeat=repeat, timer=time.process_time, globals=globals()\n",
    "    # )\n",
    "\n",
    "    # check = []\n",
    "    # with open(\"test_class_spacy.vrt\", \"r\") as file:\n",
    "    #     for line in file:\n",
    "    #         if not line.startswith(\"!\") and not line.startswith(\"<\"):\n",
    "    #             check.append(int(line.split()[0]))\n",
    "\n",
    "    # for i, elem in enumerate(check):\n",
    "    #     if i > 0:\n",
    "    #         try:\n",
    "    #             assert elem - check[i - 1] == 1\n",
    "    #         except AssertionError:\n",
    "    #             print(i, elem)\n",
    "    # print(\"Asserted iterating indexing.\")\n",
    "\n",
    "    # print(\n",
    "    #     \"Using spacy.pipe: {:.2f}s | Iterating directly: {:.2f}s\".format(\n",
    "    #         min(Time), min(Time1)\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # # check that output is indeed the same for both methods\n",
    "    # with open(\"test_new_spacy.vrt\", \"r\") as file:\n",
    "    #     out1 = file.readlines()\n",
    "\n",
    "    # with open(\"test_class_spacy.vrt\", \"r\") as file:\n",
    "    #     out = file.readlines()\n",
    "\n",
    "    # i = 0\n",
    "    # equal = True\n",
    "    # for i, line in enumerate(out):\n",
    "    #     if not line.startswith(\"!\"):\n",
    "    #         try:\n",
    "    #             assert line == out1[i]\n",
    "    #         except AssertionError:\n",
    "    #             if i < 10:\n",
    "    #                 print(line, out1[i])\n",
    "    #                 equal = False\n",
    "    #                 i += 1\n",
    "    # print(\"Asserted equality\")\n",
    "\n",
    "\n",
    "# with open(\"out/test_spacy.vrt\", \"r\") as file:\n",
    "# for line in file:\n",
    "# check if vrt file was written correctly\n",
    "# lines with \"!\" are comments, <s> and </s> mark beginning and\n",
    "# end of sentence, respectively\n",
    "# if line != \"<s>\\n\" and line != \"</s>\\n\" and line.split()[0] != \"!\":\n",
    "# try:\n",
    "#    assert len(line.split()) == len(spacy_dict[\"processors\"].split(\",\"))\n",
    "# except AssertionError:\n",
    "#    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mstanza main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dict = be.prepare_run.load_input_dict(\"./src/annotator/input\")\n",
    "    # take only the part of dict pertaining to stanza\n",
    "    stanza_dict = dict[\"stanza_dict\"]\n",
    "    # to point to user-defined model directories\n",
    "    # stanza does not accommodate fully at the moment\n",
    "    mydict = mstanza_preprocess.fix_dict_path(stanza_dict)\n",
    "    print(stanza_dict)\n",
    "    print(mydict)\n",
    "    # stanza does not care about the extra comment keys\n",
    "    # but we remove them for subsequent processing just in case\n",
    "    # now we need to select the processors and \"activate\" the sub-dictionaries\n",
    "    mydict = be.prepare_run.update_dict(mydict)\n",
    "    mydict = be.prepare_run.activate_procs(mydict, \"stanza_\")\n",
    "    mytext = be.prepare_run.get_sample_text()\n",
    "    # mytext = \"This is an example. And here we go.\"\n",
    "    # initialize instance of the class\n",
    "    obj = mstanza_pipeline(mydict)\n",
    "    obj.init_pipeline()\n",
    "    out = obj.process_text(mytext)\n",
    "    obj.postprocess()\n",
    "    # For the output:\n",
    "    # We need a module that transforms a generic dict into xml."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "248d8c9983699a243ff8e6a68a4f537cc73e09b2b8d6acc84fd600ae9279441d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('cwbproj': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
