{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook for annotator package\n",
    "## Scientific Software Center, I. S. Ulusoy, C. Delavier, Heidelberg University\n",
    "*January 2022*\n",
    "\n",
    "In the following the basic functionalities of the package are introduced. We will load basic text in English and German and annotate it using the available features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the modules of the annotator package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import annotator.base as be\n",
    "import annotator.mspacy as msp\n",
    "import annotator.mstanza as sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is passed to the package using a dictionary (json-file). Later, this will be hidden in the user interface. For now, you will load the default dictionary which has all options pre-set to default values, and then replace the options that you specify for your desired processing.\n",
    "\n",
    "### Read the default dictionary - do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in input.json\n",
    "default_dict = be.prepare_run.load_input_dict(\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main input dictionary contains three important parameters:\n",
    "```\n",
    "    \"input\": \"./test/test_files/example_en.txt\",\n",
    "    \"output\": \"out/output_en\",\n",
    "    \"tool\": \"spacy\",\n",
    "```\n",
    "You can print these using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(default_dict[\"input\"])\n",
    "print(default_dict[\"output\"])\n",
    "print(default_dict[\"tool\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tell the programm that the data we want to annotate is stored in `example_en.txt` with path to the file `./test/test_files/`, that we want to output to a file which we can identify as `output_en` in the folder `out` and that we want to use the tool `spacy` to annotate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the keys for your specific run - do change\n",
    "\n",
    "You can now modify the keys to specify a different input file, output file, and selected tool as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a copy of the dictionary for your run\n",
    "mydict = default_dict\n",
    "# now you need to set your parameters\n",
    "# change the value of the key on the right hand of the \"=\"\n",
    "mydict[\"input\"] = \"./test/test_files/example_en.txt\"\n",
    "# change the value of the key on the right hand of the \"=\"\n",
    "mydict[\"output\"] = \"output_en\"\n",
    "mydict[\"tool\"] = \"spacy\"  # or \"stanza\" - so far, only spacy and stanza are implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the \"\" around the keys - these are essential as the values are passed as string and should not be removed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the input text to be processed as raw text - do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the raw text\n",
    "data = be.prepare_run.get_text(mydict[\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print the text as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also directly copy and paste text here - take care that it is surrounded by double quotes again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"This is my text. I like it better this way.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the text using SpaCy\n",
    "More information about SpaCy is found [here](https://spacy.io/). Generally, SpaCy supports [these languages](https://spacy.io/usage/models), but at the moment only English and German are available in the annotator package. We will add more languages based on your requests - so please get in touch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR - the essence\n",
    "You need to specify which language you would like to process. It is also good to specify the processing options, like tokenization, part-of-speech, lemma, etc., although if not specified the package will select all that are available for the language. There is currently a restriction: The output that is generated in the end and passed to cwb can only contain one or several of these options: sentencize, tokenize, part-of-speech, lemma. All other options do get processed but are not written to the file yet. Here we need some more feedback on the format that is required for cwb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which language has been selected\n",
    "print(mydict[\"spacy_dict\"][\"lang\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the selected language as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict[\"spacy_dict\"][\"lang\"] = \"en\"\n",
    "# currently, only \"en\" and \"de\" are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out which model is being used\n",
    "print(mydict[\"spacy_dict\"][\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to change the model, if another one has been downloaded. At the moment, only `en_core_web_md` and `de_core_news_md` are available. We will add more upon request, so please get in touch!\n",
    "\n",
    "Now select the processors that you would like to use: For the default English pipeline, the available options are `tok2vec, senter, tagger, parser, attribute_ruler, lemmatizer, ner`, where the first two options are required for tokenization, and the other options are: [Dependency parser](https://spacy.io/api/dependencyparser), POS-tagging via the [attribute ruler](https://spacy.io/api/attributeruler), [lemma](https://spacy.io/api/lemmatizer), and [named-entity recognition](https://spacy.io/api/entityrecognizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which processors have been selected\n",
    "print(mydict[\"spacy_dict\"][\"processors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these if you like as so\n",
    "mydict[\"spacy_dict\"][\n",
    "    \"processors\"\n",
    "] = \"tok2vec, senter, tagger, attribute_ruler, lemmatizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In more detail for the interested user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be more tools to chose from, but for simplicity their configurations have been stripped for now. The spacy specific config is found in the `\"spacy_dict\"` section of the main input dictionary. Here we find the parameters we can tell spacy to enable it to annotate the data. The entries do usually come with a comment explaining what the parameters do. Lets look through the ones we set up in input.json:\n",
    "```\n",
    "    \"model\": false,\n",
    "```\n",
    "Here we can specify a model spacy should use to annotate the text if we want to. We leave it to false for now though.\n",
    "```\n",
    "    \"lang\": \"en\",\n",
    "```\n",
    "Here we specify that the language of the data we want to annotate is english. Since we didn't specify a model this information will be needed to chose one for us.\n",
    "```\n",
    "    \"text_type\": \"news\",\n",
    "```\n",
    "We specify what kind of text we want to annotate in order to chose an appropriate model for the task. This does currently only support \"news\" for english. The setup we chose here will lead to the usage of the model `en_core_web_md`.\n",
    "```\n",
    "    \"processors\": \"tok2vec, senter, tagger, parser, attribute_ruler, lemmatizer, ner\",\n",
    "```\n",
    "Here we specifiy the processors for the pipeline we will apply to our data. This will define what kind of annotations we get in the end, as well as potentionally impacting performance of the pipeline. The availability of specific processors is dependent upon the selected model. The module checks if all requested processors are available before trying to load them and should tell us if there is a problem. Available pipeline components for pretrained `spacy models` can be found in the [spacy models documentation](https://spacy.io/models).\n",
    "\n",
    "The remaining entries are not immediatly important for this example and are all set to their default values. Especially the `\"config\"` parameter and its contents are defined for a given pretrained model in it's config.cfg file, changing this is not recommended unless you really know what you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we would load the tool as specified by the .json. In this case we would load the spacy pipeline from the `mspacy` module. We are told what components we load for our pipeline and which model we are using. In our case we load the models `en_core_web_md` and `de_core_news_md` with all their pipeline components. The function calls below create `spacy_pipe` objects which we can than apply to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the SpaCy pipeline and process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pipeline from the config\n",
    "pipe = msp.spacy_pipe(mydict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing this we only have to apply the pipeline to the data we read in earlier. For this we use the `apply_to` function of the `spacy_pipe` object, this generates the annotated `spacy.Doc` which is stored in our `spacy_pipe` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pipeline to data\n",
    "annotated = pipe.apply_to(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the results of the pipeline  we can easily pass the results to a .vrt file using the output name defined in the .json. This is done by using the `pass_results` function build into the `spacy_pipe` object. Doing it this way, we also directly encode our results for `CWB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the annotated .vrt and pass to cwb\n",
    "annotated.pass_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pipeline, applying it and passing the results can be done conveniently in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msp.spacy_pipe(mydict).apply_to(data).pass_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the newly annotated corpus in cwb via cwb-ccc\n",
    "This needs to be adjusted on the jupyterjub as there are specific directories required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccc import Corpora\n",
    "\n",
    "corpora = Corpora(\n",
    "    cqp_bin=\"/usr/local/cwb-3.4.22/bin/cqp\",\n",
    "    registry_path=\"/home/jovyan/DemoCorpus-German/registry\",\n",
    ")\n",
    "print(corpora)\n",
    "corpora.show()\n",
    "# needed to provide absolute paths in registry for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the newly encoded corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a different tool: stanza\n",
    "In this section we will be using a different tool to annotate our data. The only other available tool at this moment is `stanza`. Looking at the default dictionary, we now set\n",
    "```\n",
    "    \"tool\": \"stanza\",\n",
    "```\n",
    "which would indicate to the program that we do indeed want to use `stanza`. As data we will again use `example_en.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a copy of the dictionary for your run\n",
    "mydict = default_dict\n",
    "# now you need to set your parameters\n",
    "# change the value of the key on the right hand of the \"=\"\n",
    "mydict[\"input\"] = \"./test/test_files/example_de.txt\"\n",
    "# change the value of the key on the right hand of the \"=\"\n",
    "mydict[\"output\"] = \"output_de\"\n",
    "mydict[\"tool\"] = \"stanza\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza input options\n",
    "For the processing with [stanza](https://stanfordnlp.github.io/stanza/), we only pass the stanza-specific part of the dictionary to the pipeline and need to clean the dictionary before using. Currently, only tokenization, POS and lemma are implemented for the same reasons as above. German requires also `mwt`, but the multi-word expressions are not marked as such in the generated output file. For these, p-attributes will be included at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relevant part of dict for stanza\n",
    "stanza_dict = mydict[\"stanza_dict\"]\n",
    "\n",
    "# remove the comments\n",
    "stanza_dict = be.prepare_run.update_dict(stanza_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language is selected as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which option is set\n",
    "print(stanza_dict[\"lang\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to a different value - currently only \"en\" and \"de\"\n",
    "stanza_dict[\"lang\"] = \"de\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processors may be set as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which options are selected\n",
    "print(stanza_dict[\"processors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to different values\n",
    "# please note that stanza does not allow spaces between the processor names\n",
    "stanza_dict[\"processors\"] = \"tokenize,pos,mwt,lemma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the processors requested in the input, we have to activate them for `stanza`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate processors\n",
    "stanza_dict = be.prepare_run.activate_procs(stanza_dict, \"stanza_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input text\n",
    "data = be.prepare_run.get_text(mydict[\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may again look at the raw text like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can set it manually as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"Hier ist nun das zweite Beispiel. Wir können momentan nur reinen Text einlesen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the dictionary we can create a `mstanza_pipeline` object from the `mstanza` module. Please request additional models to the English and German ones that are currently installed. We will then add them to the Hub and you do not need to worry about downloads. For a list of available languages and models, see [here](https://stanfordnlp.github.io/stanza/available_models.html). \n",
    "\n",
    "If you want to use your own custom model, youhave to change the path set in `\"dir\"` to point to your model. \n",
    "\n",
    "If you want to download another model, execute the code below while replacing the model language to your selected language. Note that the models have sizes of several hundred MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza import download\n",
    "\n",
    "download(\"en\", model_dir=\"/home/jovyan/shared/stanza_resources\")\n",
    "stanza_dict[\"dir\"] = \"/home/jovyan/shared/stanza_resources/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can initialize the text processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the pipeline with the dict\n",
    "stanza_pipe = sa.mstanza_pipeline(stanza_dict)\n",
    "\n",
    "# to get the working pipeline we have to use the inbuilt initialize function\n",
    "stanza_pipe.init_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply the pipeline to our data through the `process_text` function of the `mstanza_pipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pipeline to data\n",
    "results = stanza_pipe.process_text(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the results to a file and export to `CWB` we can then use the `mstanza_pipeline.postprocess` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_pipe.postprocess(mydict[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the newly annotated corpus can be used in cwb/cqp."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f780981b168fa18ba6e0a1a6495d97f80f01fc3bd26211c5d21403ec246f1ca7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('arg': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
