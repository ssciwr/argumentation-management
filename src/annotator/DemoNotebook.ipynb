{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook for annotator package\n",
    "In the following the basic functionalities of the package are introduced. We will load basic text in english and german and annotate it using the available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import annotator.base as be\n",
    "# import annotator.mspacy as msp\n",
    "# import annotator.mstanza as sa\n",
    "\n",
    "import base as be\n",
    "import mspacy as msp\n",
    "import mstanza as sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The input for spacy\n",
    "The input for the package would normally be set up in an input.json file passed to the package.  There are two example .json files in this directory which we will use. Lets look at the contents of `example_en.json`. The first few parameters we encounter are:\n",
    "```\n",
    "    \"input\": \"example_en.txt\",\n",
    "    \"output\": \"output_en\",\n",
    "    \"tool\": \"spacy\",\n",
    "```\n",
    "These tell the programm that the data we want to annotate is stored in `example_en.txt`, that we want to output to a file which we can identify as `output_en` and that we want to use the tool `spacy` to annotate the data. There will be more tools to chose from, but for simplicity their configurations have been stripped for now. The spacy specific config is found in the `\"spacy_dict\"` section. Here we find the parameters we can tell spacy to enable it to annotate the data. The entries do usually come with a comment explaining what the parameters do. Lets look through the ones we set up in example_en.json:\n",
    "```\n",
    "    \"model\": false,\n",
    "```\n",
    "Here we can specify a model spacy should use to annotate the text if we want to. We leave it to false for now though.\n",
    "```\n",
    "    \"lang\": \"en\",\n",
    "```\n",
    "Here we specify that the language of the data we want to annotate is english. Since we didn't specify a model this information will be needed to chose one for us.\n",
    "```\n",
    "    \"text_type\": \"news\",\n",
    "```\n",
    "We specify what kind of text we want to annotate in order to chose an appropriate model for the task. This does currently only support \"news\" for english. The setup we chose here will lead to the usage of the model `en_core_web_md`.\n",
    "```\n",
    "    \"processors\": \"tok2vec, senter, tagger, parser, attribute_ruler, lemmatizer, ner\",\n",
    "```\n",
    "Here we specifiy the processors for the pipeline we will apply to our data. This will define what kind of annotations we get in the end, aswell as potentionally impacting performance of the pipeline. The availability of specific processors is depenant upon the selected model. The module checks if all requested processors are available before trying to load them and should tell us if there is a problem. Available pipeline components for pretrained `spacy models` can be found in the [spacy models documentation](https://spacy.io/models).\n",
    "\n",
    "The remaining entries are not immediatly important for this example and are all set to their default values. Especially the `\"config\"` parameter and its contents are defined for a given pretrained model in it's config.cfg file, messing with this is usually not recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running based of the .json\n",
    "Let's now look at what the program would do with the supplied information from the .json input files. First we would read in the .json files to make them available as dictionaries. We can use the `load_input_dict` function from the `prepare_run` class of the `base` module to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in example_en.json\n",
    "dict_en = be.prepare_run.load_input_dict(\"example_en\")\n",
    "# print(dict_en)\n",
    "\n",
    "# read in example_de.json\n",
    "dict_de = be.prepare_run.load_input_dict(\"example_de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we habe access to the information from the .json files we can load in the data from the specified locations. We can once again use a function from `base.prepare_run` called `get_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the english example text from example_en.txt\n",
    "data_en = be.prepare_run.get_text(dict_en[\"input\"])\n",
    "# print(data_en)\n",
    "\n",
    "# read in the german example text from example_de.txt\n",
    "data_de = be.prepare_run.get_text(dict_de[\"input\"])\n",
    "# print(data_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we would load the tool as specified by the .json. In this case we would load the spacy pipeline from the `mspacy` module. We are told what components we load for our pipeline and which model we are using. In our case we load the models `en_core_web_md` and `de_core_news_md` with all their pipeline components. The function calls below create `spacy_pipe` objects which we can than apply to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pipeline from the config\n",
    "pipe_en = msp.spacy_pipe(dict_en)\n",
    "\n",
    "pipe_de = msp.spacy_pipe(dict_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing this we only have to apply the pipeline to the data we read in earlier. For this we use the `apply_to` function of the `spacy_pipe` object, this generates the annotated `spacy.Doc` which is stored in our `spacy_pipe` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pipeline to data\n",
    "annotated_en = pipe_en.apply_to(data_en)\n",
    "\n",
    "annotated_de = pipe_de.apply_to(data_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the results of the pipeline  we can easily pass the results to a .vrt file using the output name defined in the .json. This is done by using the `pass_results` function build into the `spacy_pipe` object. Doing it this way, we also directly encode our results for `CWB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the annotated .vrt and pass to cwb\n",
    "annotated_en.pass_results()\n",
    "\n",
    "annotated_de.pass_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pipeline, applying it and passing the results can be done conveniently in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for english data\n",
    "msp.spacy_pipe(dict_en).apply_to(data_en).pass_results()\n",
    "\n",
    "# is equivalent to the above\n",
    "# pipe_en = msp.spacy_pipe(dict_en)\n",
    "# annotated_en = pipe_en.apply_to(data_en)\n",
    "# annotated_en.pass_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a different tool\n",
    "In this section we will be using a different tool to annotate our data. The only other available tool at this moment is `stanza`. Looking at `example_stanza.json` we see that we now have set\n",
    "```\n",
    "    \"tool\": \"stanza\",\n",
    "```\n",
    "which woul indicate to the program that we do indeed want to use `stanza`. As data we will again use `example_en.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dict\n",
    "dict_sa = be.prepare_run.load_input_dict(\"example_stanza\")\n",
    "\n",
    "outname = dict_sa[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relevant part of dict for stanza\n",
    "stanza_dict = dict_sa[\"stanza_dict\"]\n",
    "\n",
    "# remove the comments\n",
    "stanza_dict = be.prepare_run.update_dict(stanza_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the processors requested in `example_stanza.json` we have to activate them for `stanza`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate processors\n",
    "stanza_dict = be.prepare_run.activate_procs(stanza_dict, \"stanza_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still have the data from before, but will just get it again\n",
    "data_en = be.prepare_run.get_text(\"example_en.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the dictionary we can create a `mstanza_pipeline` object from the `mstanza` module. Notice that you might have to change the path set in `\"dir\"` to point to your installation of the `stanza` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to download the model to the needed path execute the code below.\n",
    "# Note that the model has a size of several hundred MB.\n",
    "\n",
    "# from stanza import download\n",
    "# download(\"en\", model_dir = \"test/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the pipeline with the dict\n",
    "stanza_pipe = sa.mstanza_pipeline(stanza_dict)\n",
    "\n",
    "# to get the working pipeline we have to use the inbuilt initialize function\n",
    "stanza_pipe.init_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then apply the pipeline to our data through the `process_text` function of the `mstanza_pipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pipeline to data\n",
    "results = stanza_pipe.process_text(data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the results to a file and export to `CWB` we can then use the `mstanza_pipeline.postprocess` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_pipe.postprocess(outname)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "248d8c9983699a243ff8e6a68a4f537cc73e09b2b8d6acc84fd600ae9279441d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('cwbproj': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
