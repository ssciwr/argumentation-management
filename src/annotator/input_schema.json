{   
    "properties": {
        "input": {
            "default": "input.txt",
            "title": "Input file:",
            "type": "string"
        },
        "output": {
            "default": "out/output",
            "title": "Output directory:",
            "type": "string"
        },
        "cwb_dict": {
            "properties": {
                "corpus_name": {
                    "default": "test",
                    "title": "Name of corpus to be annotated:",
                    "type": "string"
                },
                "corpus_dir": {
                    "default": "/home/jovyan/shared/corpora/",
                    "title": "Name of corpora directory for cwb:",
                    "type": "string"
                },
                "registry_dir": {
                    "default": "/home/jovyan/shared/registry/",
                    "title": "Name of registry directory for cwb:",
                    "type": "string"
                }
            },
            "title": "Parameters for CWB encoding",
            "type": "object"
        },
        "tooldict": {
            "anyOf": [
              {
                "properties": {
                  "lang": {
                    "default": "en",
                    "description": "Language code for the language to build the Pipeline in",
                    "enum": [
                      "en",
                      "de"
                    ],
                    "title": "Language",
                    "type": "string"
                  },
                  "dir": {
                    "default": "./test/models",
                    "description": "Directory where models are stored - this maynot work for pipeline",
                    "title": "Model directory:",
                    "type": "string"
                  },
                  "package": {
                    "default": "default",
                    "description": "Using the default model, for others see: https://stanfordnlp.github.io/stanza/models.html",
                    "title": "Model to use:",
                    "type": "string"
                  },
                  "processors": {
                    "default": "tokenize, pos, lemma",
                    "description": "Comma-separated list of processors to use, can also be given as a dictionary: {'tokenize': 'ewt', 'pos': 'ewt'}",
                    "title": "Processors to use:",
                    "type": "string"
                  },
                  "logging_level": {
                    "default": "INFO",
                    "description": "DEBUG, INFO, WARN, ERROR, CRITICAL, FATAL; FATAL has least amount of log info printed",
                    "title": "Level of logging:",
                    "type": "string"
                  },
                  "verbose": {
                    "default": true,
                    "description": "True corresponds to INFO, False corresponds to ERROR",
                    "title": "Verbose? (True corresponds to INFO, False corresponds to ERROR)",
                    "type": "boolean" 
                  },
                  "type": {
                    "const": "stanza",
                    "type": "string"
                  },
                  "use_GPU": {
                    "default": false,
                    "description": "Use GPU if available, False forces CPU only",
                    "title": "Use GPU (Use GPU if available, False forces CPU only)",
                    "type": "boolean"
                  },
                  "stanza_tokenize": {
                      "properties": {
                            "tokenize_batch_size": {
                                "default": 32,
                                "description": "Maximum number of paragraphs to process as minibatch for efficient processing",
                                "title": "Tokenize batch size",
                                "type": "integer",
                                "minimum": 1
                            },
                            "tokenize_no_ssplit": {
                                "default": false,
                                "description": "",
                                "title": "Toggle tokenize no-split.",
                                "type": "boolean"
                            }
                      },
                      "description": "Tokenizes the text and performs sentence segmentation, dependency: -",
                      "title": "Stanza Tokenizer",
                      "type": "object"
                    },
                 "stanza_mwt": {
                     "properties": {
                        "mwt_batch_size": {
                            "default": 50,
                            "description": " Specifies maximum number of words to process as a minibatch",
                            "title": "Number of words to process per minibatch.",
                            "type": "integer",
                            "minimum": 1
                        }
                     },
                     "description": "Expands multi-word tokens (MWT) predicted by the TokenizeProcessor, this is only applicable to some languages, dependency: - 'tokenize'",
                     "title": "Stanza multi-word tokens",
                     "type": "object"
                 },
                 "stanza_lemma" : {
                    "properties": {
                        "lemma_use_identity": {
                            "default": false,
                            "description": "Identity lemmatizer is used instead of statistical lemmatizer",
                            "title": "Toggle identity lemmatizer",
                            "type": "boolean"
                        },
                        "lemma_batch_size": {
                            "default": 50,
                            "description": "Maximum number of words to batch for efficient processing. Default: 50",
                            "title": "Lemmatizer batch size",
                            "type": "integer",
                            "minimum": 1
                        },
                        "lemma_ensemble_dict": {
                            "default": true,
                            "description": "Lemmatizer will ensemble a seq2seq model with output from dictionary-based lemmatizer, improvement for many languages",
                            "title": "Enable seq2seq model",
                            "type": "boolean"
                        },
                        "lemma_dict_only": {
                            "default": true,
                            "description": "Dictionary-based lemmatizer only",
                            "title": "Toggle dictionary-based lemmatize",
                            "type": "boolean"
                        },
                        "lemma_edit": {
                            "default": true,
                            "description": "Use an edit classifier in addition to seq2seq to make long sentence predictions more stable",
                            "title": "Toggle edit classifier",
                            "type": "boolean"
                        },
                        "lemma_beam_size": {
                            "default": 1,
                            "description": "Control beam size during decoding in seq2seq",
                            "title": "Control beam size during decoding in seq2seq",
                            "type": "integer",
                            "minimum": 1
                        },
                        "lemma_max_dec_len": {
                            "default": 50,
                            "description": "Control maximum decoding character length in seq2seq, decoder will stop if this length is achieved and end-of-sequence character still not seen",
                            "title": "Control maximum decoding character length in seq2seq",
                            "type": "integer",
                            "minimum": 1
                        }
                    },
                    "description": "Generates the word lemmas for all words in the Document, dependency: - 'tokenize, mwt, pos'",
                    "title": "Stanza lemma",
                    "type": "object"
                 },
                 "stanza_depparse": {
                     "properties": {
                        "depparse_batch_size": {
                            "default": 5000,
                            "description": "Maximum number of words to process as minibatch, may require large amounts of RAM; should be set larger than the number of workds in the longest sentence in input document, or may result in unexpected behaviors",
                            "title": "Maximum number of words per minibatch",
                            "type": "integer",
                            "minimum": 1
                        },
                        "depparse_pretagged": {
                            "default": false,
                            "description": "Assumes document is tokenized and pretagged, only run dependency parsing",
                            "title": "Assume document is pretokenized and pretagged",
                            "type": "boolean"
                        }
                     },
                     "description": "Provides an accurate syntactic dependency parsing analysis, dependency: - 'tokenize, mwt, pos, lemma'",
                     "title": "Stanza depparse",
                     "type": "object"
                 },
                 "stanza_ner": {
                     "properties": {
                        "ner_batch_size": {
                            "default": 32,
                            "description": "Maximum number of sentences to process as minibatch, may require large amounts of RAM",
                            "title": "Maximum number of words per minibatch",
                            "type": "integer",
                            "minimum": 1
                        }
                     },
                     "description": "Recognize named entities for all token spans in the corpus, dependency: - 'tokenize, mwt'",
                     "title": "Stanza ner",
                     "type": "object"
                 },
                 "stanza_sentiment": {
                        "oneOf": [
                            {
                                "properties": {
                                    "batch_size": {
                                        "type": "null",
                                        "default": null,
                                        "title": "Process whole corpus at once."
                                    }
                                },
                                "title": "Apply stanza-sentiment to complete corpus at once.",
                                "type": "object" 
                            },
                            {
                                "properties": {
                                    "batch_size": {
                                        "type": "integer",
                                        "default": 2,
                                        "title": "Number of chunks:",
                                        "minimum": 2
                                    }
                                },
                                "title": "Chunk corpus for Stanza-sentiment.",
                                "type": "object"
                            }
                        ],
                        "description": "Assign per-sentence sentiment scores, dependency: - 'tokenize, mwt'"

                 },
                 "stanza_constituency": {
                     "properties": {
                     },
                     "description": "Parse each sentence in a document using a phrase structure parser, dependency: - 'tokenize, mwt, pos'",
                     "title": "Stanza constituenty",
                     "type": "object"
                 }
                },
                "title": "Stanza",
                "type": "object"
              },
              
              {
                "properties": {
                  "model": {
                    "default": "en_core_web_md",
                    "type": "string"
                  },
                  "type": {
                    "const": "spacy",
                    "type": "string"
                  }
                },
                "title": "Spacy",
                "type": "object"
              },
              {
                "properties": {
                  "lang": {
                    "default": "en",
                    "description": "Language code for the language to use",
                    "enum": [
                      "en",
                      "de"
                    ],
                    "title": "Language",
                    "type": "string"
                  },
                  "type": {
                    "const": "flair",
                    "type": "string"
                  }
                },
                "title": "Flair",
                "type": "object"
              }
            ],
            "title": "Tool to use for annotation:"
          }
        
    },
    "type": "object"
}